{"name":"io.debezium.connector.oracle.OracleConnector","error_count":2,"groups":["Common","Transforms","Predicates","Error Handling","Topic Creation","Exactly Once Support","offsets.topic","Transforms: route","Oracle","Connector","History Storage","Events"],"configs":[{"definition":{"name":"name","type":"STRING","required":true,"default_value":null,"importance":"HIGH","documentation":"Globally unique name to use for this connector.","group":"Common","width":"MEDIUM","display_name":"Connector name","dependents":[],"order":1},"value":{"name":"name","value":null,"recommended_values":[],"errors":["Missing required configuration \"name\" which has no default value."],"visible":true}},{"definition":{"name":"connector.class","type":"STRING","required":true,"default_value":null,"importance":"HIGH","documentation":"Name or alias of the class for this connector. Must be a subclass of org.apache.kafka.connect.connector.Connector. If the connector is org.apache.kafka.connect.file.FileStreamSinkConnector, you can either specify this full name,  or use \"FileStreamSink\" or \"FileStreamSinkConnector\" to make the configuration a bit shorter","group":"Common","width":"LONG","display_name":"Connector class","dependents":[],"order":2},"value":{"name":"connector.class","value":"io.debezium.connector.oracle.OracleConnector","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"tasks.max","type":"INT","required":false,"default_value":"1","importance":"HIGH","documentation":"Maximum number of tasks to use for this connector.","group":"Common","width":"SHORT","display_name":"Tasks max","dependents":[],"order":3},"value":{"name":"tasks.max","value":"1","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"key.converter","type":"CLASS","required":false,"default_value":null,"importance":"LOW","documentation":"Converter class used to convert between Kafka Connect format and the serialized form that is written to Kafka. This controls the format of the keys in messages written to or read from Kafka, and since this is independent of connectors it allows any connector to work with any serialization format. Examples of common formats include JSON and Avro.","group":"Common","width":"SHORT","display_name":"Key converter class","dependents":[],"order":4},"value":{"name":"key.converter","value":"org.apache.kafka.connect.json.JsonConverter","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"value.converter","type":"CLASS","required":false,"default_value":null,"importance":"LOW","documentation":"Converter class used to convert between Kafka Connect format and the serialized form that is written to Kafka. This controls the format of the values in messages written to or read from Kafka, and since this is independent of connectors it allows any connector to work with any serialization format. Examples of common formats include JSON and Avro.","group":"Common","width":"SHORT","display_name":"Value converter class","dependents":[],"order":5},"value":{"name":"value.converter","value":"io.apicurio.registry.utils.converter.AvroConverter","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"header.converter","type":"CLASS","required":false,"default_value":null,"importance":"LOW","documentation":"HeaderConverter class used to convert between Kafka Connect format and the serialized form that is written to Kafka. This controls the format of the header values in messages written to or read from Kafka, and since this is independent of connectors it allows any connector to work with any serialization format. Examples of common formats include JSON and Avro. By default, the SimpleHeaderConverter is used to serialize header values to strings and deserialize them by inferring the schemas.","group":"Common","width":"SHORT","display_name":"Header converter class","dependents":[],"order":6},"value":{"name":"header.converter","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"transforms","type":"LIST","required":false,"default_value":"","importance":"LOW","documentation":"Aliases for the transformations to be applied to records.","group":"Transforms","width":"LONG","display_name":"Transforms","dependents":[],"order":7},"value":{"name":"transforms","value":"route","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"predicates","type":"LIST","required":false,"default_value":"","importance":"LOW","documentation":"Aliases for the predicates used by transformations.","group":"Predicates","width":"LONG","display_name":"Predicates","dependents":[],"order":8},"value":{"name":"predicates","value":"","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"config.action.reload","type":"STRING","required":false,"default_value":"restart","importance":"LOW","documentation":"The action that Connect should take on the connector when changes in external configuration providers result in a change in the connector's configuration properties. A value of 'none' indicates that Connect will do nothing. A value of 'restart' indicates that Connect should restart/reload the connector with the updated configuration properties.The restart may actually be scheduled in the future if the external configuration provider indicates that a configuration value will expire in the future.","group":"Common","width":"MEDIUM","display_name":"Reload Action","dependents":[],"order":9},"value":{"name":"config.action.reload","value":"restart","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"errors.retry.timeout","type":"LONG","required":false,"default_value":"0","importance":"MEDIUM","documentation":"The maximum duration in milliseconds that a failed operation will be reattempted. The default is 0, which means no retries will be attempted. Use -1 for infinite retries.","group":"Error Handling","width":"MEDIUM","display_name":"Retry Timeout for Errors","dependents":[],"order":1},"value":{"name":"errors.retry.timeout","value":"0","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"errors.retry.delay.max.ms","type":"LONG","required":false,"default_value":"60000","importance":"MEDIUM","documentation":"The maximum duration in milliseconds between consecutive retry attempts. Jitter will be added to the delay once this limit is reached to prevent thundering herd issues.","group":"Error Handling","width":"MEDIUM","display_name":"Maximum Delay Between Retries for Errors","dependents":[],"order":2},"value":{"name":"errors.retry.delay.max.ms","value":"60000","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"errors.tolerance","type":"STRING","required":false,"default_value":"none","importance":"MEDIUM","documentation":"Behavior for tolerating errors during connector operation. 'none' is the default value and signals that any error will result in an immediate connector task failure; 'all' changes the behavior to skip over problematic records.","group":"Error Handling","width":"SHORT","display_name":"Error Tolerance","dependents":[],"order":3},"value":{"name":"errors.tolerance","value":"none","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"errors.log.enable","type":"BOOLEAN","required":false,"default_value":"false","importance":"MEDIUM","documentation":"If true, write each error and the details of the failed operation and problematic record to the Connect application log. This is 'false' by default, so that only errors that are not tolerated are reported.","group":"Error Handling","width":"SHORT","display_name":"Log Errors","dependents":[],"order":4},"value":{"name":"errors.log.enable","value":"false","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"errors.log.include.messages","type":"BOOLEAN","required":false,"default_value":"false","importance":"MEDIUM","documentation":"Whether to include in the log the Connect record that resulted in a failure. For sink records, the topic, partition, offset, and timestamp will be logged. For source records, the key and value (and their schemas), all headers, and the timestamp, Kafka topic, Kafka partition, source partition, and source offset will be logged. This is 'false' by default, which will prevent record keys, values, and headers from being written to log files.","group":"Error Handling","width":"SHORT","display_name":"Log Error Details","dependents":[],"order":5},"value":{"name":"errors.log.include.messages","value":"false","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"topic.creation.groups","type":"LIST","required":false,"default_value":"","importance":"LOW","documentation":"Groups of configurations for topics created by source connectors","group":"Topic Creation","width":"LONG","display_name":"Topic Creation Groups","dependents":[],"order":1},"value":{"name":"topic.creation.groups","value":"","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"exactly.once.support","type":"STRING","required":false,"default_value":"requested","importance":"MEDIUM","documentation":"Permitted values are requested, required. If set to \"required\", forces a preflight check for the connector to ensure that it can provide exactly-once semantics with the given configuration. Some connectors may be capable of providing exactly-once semantics but not signal to Connect that they support this; in that case, documentation for the connector should be consulted carefully before creating it, and the value for this property should be set to \"requested\". Additionally, if the value is set to \"required\" but the worker that performs preflight validation does not have exactly-once support enabled for source connectors, requests to create or validate the connector will fail.","group":"Exactly Once Support","width":"SHORT","display_name":"Exactly once support","dependents":[],"order":2},"value":{"name":"exactly.once.support","value":"requested","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"transaction.boundary","type":"STRING","required":false,"default_value":"poll","importance":"MEDIUM","documentation":"Permitted values are: poll, interval, connector. If set to 'poll', a new producer transaction will be started and committed for every batch of records that each task from this connector provides to Connect. If set to 'connector', relies on connector-defined transaction boundaries; note that not all connectors are capable of defining their own transaction boundaries, and in that case, attempts to instantiate a connector with this value will fail. Finally, if set to 'interval', commits transactions only after a user-defined time interval has passed.","group":"Exactly Once Support","width":"SHORT","display_name":"Transaction Boundary","dependents":[],"order":3},"value":{"name":"transaction.boundary","value":"poll","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"transaction.boundary.interval.ms","type":"LONG","required":false,"default_value":null,"importance":"LOW","documentation":"If 'transaction.boundary' is set to 'interval', determines the interval for producer transaction commits by connector tasks. If unset, defaults to the value of the worker-level 'offset.flush.interval.ms' property. It has no effect if a different transaction.boundary is specified.","group":"Exactly Once Support","width":"SHORT","display_name":"Transaction boundary interval","dependents":[],"order":4},"value":{"name":"transaction.boundary.interval.ms","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"offsets.storage.topic","type":"STRING","required":false,"default_value":null,"importance":"LOW","documentation":"The name of a separate offsets topic to use for this connector. If empty or not specified, the workerâ€™s global offsets topic name will be used. If specified, the offsets topic will be created if it does not already exist on the Kafka cluster targeted by this connector (which may be different from the one used for the worker's global offsets topic if the bootstrap.servers property of the connector's producer has been overridden from the worker's). Only applicable in distributed mode; in standalone mode, setting this property will have no effect.","group":"offsets.topic","width":"LONG","display_name":"Offsets topic","dependents":[],"order":1},"value":{"name":"offsets.storage.topic","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"transforms.route.type","type":"CLASS","required":true,"default_value":null,"importance":"HIGH","documentation":"Class for the 'route' transformation.","group":"Transforms: route","width":"LONG","display_name":"Transformation type for route","dependents":[],"order":0},"value":{"name":"transforms.route.type","value":"org.apache.kafka.connect.transforms.RegexRouter","recommended_values":["io.debezium.connector.db2as400.smt.RepackageJavaFriendlySchemaRenamer","io.debezium.connector.jdbc.transforms.ConvertCloudEventToSaveableForm","io.debezium.connector.mongodb.transforms.ExtractNewDocumentState","io.debezium.connector.mongodb.transforms.outbox.MongoEventRouter","io.debezium.connector.mysql.transforms.ReadToInsertEvent","io.debezium.connector.postgresql.transforms.timescaledb.TimescaleDb","io.debezium.connector.vitess.transforms.FilterTransactionTopicRecords","io.debezium.connector.vitess.transforms.RemoveField","io.debezium.connector.vitess.transforms.UseLocalVgtid","io.debezium.transforms.ByLogicalTableRouter","io.debezium.transforms.ByLogicalTableRouter","io.debezium.transforms.ByLogicalTableRouter","io.debezium.transforms.ByLogicalTableRouter","io.debezium.transforms.ByLogicalTableRouter","io.debezium.transforms.ByLogicalTableRouter","io.debezium.transforms.ByLogicalTableRouter","io.debezium.transforms.ByLogicalTableRouter","io.debezium.transforms.ByLogicalTableRouter","io.debezium.transforms.ByLogicalTableRouter","io.debezium.transforms.ByLogicalTableRouter","io.debezium.transforms.ByLogicalTableRouter","io.debezium.transforms.ExtractChangedRecordState","io.debezium.transforms.ExtractChangedRecordState","io.debezium.transforms.ExtractChangedRecordState","io.debezium.transforms.ExtractChangedRecordState","io.debezium.transforms.ExtractChangedRecordState","io.debezium.transforms.ExtractChangedRecordState","io.debezium.transforms.ExtractChangedRecordState","io.debezium.transforms.ExtractChangedRecordState","io.debezium.transforms.ExtractChangedRecordState","io.debezium.transforms.ExtractChangedRecordState","io.debezium.transforms.ExtractChangedRecordState","io.debezium.transforms.ExtractChangedRecordState","io.debezium.transforms.ExtractNewRecordState","io.debezium.transforms.ExtractNewRecordState","io.debezium.transforms.ExtractNewRecordState","io.debezium.transforms.ExtractNewRecordState","io.debezium.transforms.ExtractNewRecordState","io.debezium.transforms.ExtractNewRecordState","io.debezium.transforms.ExtractNewRecordState","io.debezium.transforms.ExtractNewRecordState","io.debezium.transforms.ExtractNewRecordState","io.debezium.transforms.ExtractNewRecordState","io.debezium.transforms.ExtractNewRecordState","io.debezium.transforms.ExtractNewRecordState","io.debezium.transforms.ExtractSchemaToNewRecord","io.debezium.transforms.ExtractSchemaToNewRecord","io.debezium.transforms.ExtractSchemaToNewRecord","io.debezium.transforms.ExtractSchemaToNewRecord","io.debezium.transforms.ExtractSchemaToNewRecord","io.debezium.transforms.ExtractSchemaToNewRecord","io.debezium.transforms.ExtractSchemaToNewRecord","io.debezium.transforms.ExtractSchemaToNewRecord","io.debezium.transforms.ExtractSchemaToNewRecord","io.debezium.transforms.ExtractSchemaToNewRecord","io.debezium.transforms.ExtractSchemaToNewRecord","io.debezium.transforms.ExtractSchemaToNewRecord","io.debezium.transforms.HeaderToValue","io.debezium.transforms.HeaderToValue","io.debezium.transforms.HeaderToValue","io.debezium.transforms.HeaderToValue","io.debezium.transforms.HeaderToValue","io.debezium.transforms.HeaderToValue","io.debezium.transforms.HeaderToValue","io.debezium.transforms.HeaderToValue","io.debezium.transforms.HeaderToValue","io.debezium.transforms.HeaderToValue","io.debezium.transforms.HeaderToValue","io.debezium.transforms.HeaderToValue","io.debezium.transforms.SchemaChangeEventFilter","io.debezium.transforms.SchemaChangeEventFilter","io.debezium.transforms.SchemaChangeEventFilter","io.debezium.transforms.SchemaChangeEventFilter","io.debezium.transforms.SchemaChangeEventFilter","io.debezium.transforms.SchemaChangeEventFilter","io.debezium.transforms.SchemaChangeEventFilter","io.debezium.transforms.SchemaChangeEventFilter","io.debezium.transforms.SchemaChangeEventFilter","io.debezium.transforms.SchemaChangeEventFilter","io.debezium.transforms.SchemaChangeEventFilter","io.debezium.transforms.SchemaChangeEventFilter","io.debezium.transforms.TimezoneConverter","io.debezium.transforms.TimezoneConverter","io.debezium.transforms.TimezoneConverter","io.debezium.transforms.TimezoneConverter","io.debezium.transforms.TimezoneConverter","io.debezium.transforms.TimezoneConverter","io.debezium.transforms.TimezoneConverter","io.debezium.transforms.TimezoneConverter","io.debezium.transforms.TimezoneConverter","io.debezium.transforms.TimezoneConverter","io.debezium.transforms.TimezoneConverter","io.debezium.transforms.TimezoneConverter","io.debezium.transforms.outbox.EventRouter","io.debezium.transforms.outbox.EventRouter","io.debezium.transforms.outbox.EventRouter","io.debezium.transforms.outbox.EventRouter","io.debezium.transforms.outbox.EventRouter","io.debezium.transforms.outbox.EventRouter","io.debezium.transforms.outbox.EventRouter","io.debezium.transforms.outbox.EventRouter","io.debezium.transforms.outbox.EventRouter","io.debezium.transforms.outbox.EventRouter","io.debezium.transforms.outbox.EventRouter","io.debezium.transforms.outbox.EventRouter","io.debezium.transforms.partitions.PartitionRouting","io.debezium.transforms.partitions.PartitionRouting","io.debezium.transforms.partitions.PartitionRouting","io.debezium.transforms.partitions.PartitionRouting","io.debezium.transforms.partitions.PartitionRouting","io.debezium.transforms.partitions.PartitionRouting","io.debezium.transforms.partitions.PartitionRouting","io.debezium.transforms.partitions.PartitionRouting","io.debezium.transforms.partitions.PartitionRouting","io.debezium.transforms.partitions.PartitionRouting","io.debezium.transforms.partitions.PartitionRouting","io.debezium.transforms.partitions.PartitionRouting","io.debezium.transforms.tracing.ActivateTracingSpan","io.debezium.transforms.tracing.ActivateTracingSpan","io.debezium.transforms.tracing.ActivateTracingSpan","io.debezium.transforms.tracing.ActivateTracingSpan","io.debezium.transforms.tracing.ActivateTracingSpan","io.debezium.transforms.tracing.ActivateTracingSpan","io.debezium.transforms.tracing.ActivateTracingSpan","io.debezium.transforms.tracing.ActivateTracingSpan","io.debezium.transforms.tracing.ActivateTracingSpan","io.debezium.transforms.tracing.ActivateTracingSpan","io.debezium.transforms.tracing.ActivateTracingSpan","io.debezium.transforms.tracing.ActivateTracingSpan","org.apache.kafka.connect.transforms.Cast$Key","org.apache.kafka.connect.transforms.Cast$Value","org.apache.kafka.connect.transforms.DropHeaders","org.apache.kafka.connect.transforms.ExtractField$Key","org.apache.kafka.connect.transforms.ExtractField$Value","org.apache.kafka.connect.transforms.Filter","org.apache.kafka.connect.transforms.Flatten$Key","org.apache.kafka.connect.transforms.Flatten$Value","org.apache.kafka.connect.transforms.HeaderFrom$Key","org.apache.kafka.connect.transforms.HeaderFrom$Value","org.apache.kafka.connect.transforms.HoistField$Key","org.apache.kafka.connect.transforms.HoistField$Value","org.apache.kafka.connect.transforms.InsertField$Key","org.apache.kafka.connect.transforms.InsertField$Value","org.apache.kafka.connect.transforms.InsertHeader","org.apache.kafka.connect.transforms.MaskField$Key","org.apache.kafka.connect.transforms.MaskField$Value","org.apache.kafka.connect.transforms.RegexRouter","org.apache.kafka.connect.transforms.ReplaceField$Key","org.apache.kafka.connect.transforms.ReplaceField$Value","org.apache.kafka.connect.transforms.SetSchemaMetadata$Key","org.apache.kafka.connect.transforms.SetSchemaMetadata$Value","org.apache.kafka.connect.transforms.TimestampConverter$Key","org.apache.kafka.connect.transforms.TimestampConverter$Value","org.apache.kafka.connect.transforms.TimestampRouter","org.apache.kafka.connect.transforms.ValueToKey"],"errors":[],"visible":true}},{"definition":{"name":"transforms.route.regex","type":"STRING","required":true,"default_value":null,"importance":"HIGH","documentation":"Regular expression to use for matching.","group":"Transforms: route","width":"NONE","display_name":"regex","dependents":[],"order":1},"value":{"name":"transforms.route.regex","value":"^oracle_esw\\.ESW\\.(.*)$","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"transforms.route.replacement","type":"STRING","required":true,"default_value":null,"importance":"HIGH","documentation":"Replacement string.","group":"Transforms: route","width":"NONE","display_name":"replacement","dependents":[],"order":2},"value":{"name":"transforms.route.replacement","value":"raw-$1","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"transforms.route.negate","type":"BOOLEAN","required":false,"default_value":"false","importance":"MEDIUM","documentation":"Whether the configured predicate should be negated.","group":"Transforms: route","width":"NONE","display_name":"negate","dependents":[],"order":3},"value":{"name":"transforms.route.negate","value":"false","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"transforms.route.predicate","type":"STRING","required":false,"default_value":null,"importance":"MEDIUM","documentation":"The alias of a predicate used to determine whether to apply this transformation.","group":"Transforms: route","width":"NONE","display_name":"predicate","dependents":[],"order":4},"value":{"name":"transforms.route.predicate","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"topic.prefix","type":"STRING","required":false,"default_value":null,"importance":"HIGH","documentation":"Topic prefix that identifies and provides a namespace for the particular database server/cluster is capturing changes. The topic prefix should be unique across all other connectors, since it is used as a prefix for all Kafka topic names that receive events emitted by this connector. Only alphanumeric characters, hyphens, dots and underscores must be accepted.","group":"Oracle","width":"MEDIUM","display_name":"Topic prefix","dependents":[],"order":1},"value":{"name":"topic.prefix","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"database.hostname","type":"STRING","required":false,"default_value":null,"importance":"HIGH","documentation":"Resolvable hostname or IP address of the database server.","group":"Oracle","width":"MEDIUM","display_name":"Hostname","dependents":[],"order":2},"value":{"name":"database.hostname","value":null,"recommended_values":[],"errors":["Unable to connect: Failed to resolve Oracle database version"],"visible":true}},{"definition":{"name":"database.port","type":"INT","required":false,"default_value":"1528","importance":"HIGH","documentation":"Port of the database server.","group":"Oracle","width":"SHORT","display_name":"Port","dependents":[],"order":3},"value":{"name":"database.port","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"database.user","type":"STRING","required":false,"default_value":null,"importance":"HIGH","documentation":"Name of the database user to be used when connecting to the database.","group":"Oracle","width":"SHORT","display_name":"User","dependents":[],"order":4},"value":{"name":"database.user","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"database.password","type":"PASSWORD","required":false,"default_value":null,"importance":"HIGH","documentation":"Password of the database user to be used when connecting to the database.","group":"Oracle","width":"SHORT","display_name":"Password","dependents":[],"order":5},"value":{"name":"database.password","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"database.dbname","type":"STRING","required":false,"default_value":null,"importance":"HIGH","documentation":"The name of the database from which the connector should capture changes","group":"Oracle","width":"MEDIUM","display_name":"Database","dependents":[],"order":6},"value":{"name":"database.dbname","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"database.query.timeout.ms","type":"INT","required":false,"default_value":"600000","importance":"LOW","documentation":"Time to wait for a query to execute, given in milliseconds. Defaults to 600 seconds (600,000 ms); zero means there is no limit.","group":"Oracle","width":"NONE","display_name":"Query timeout","dependents":[],"order":7},"value":{"name":"database.query.timeout.ms","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"database.pdb.name","type":"STRING","required":false,"default_value":null,"importance":"HIGH","documentation":"Name of the pluggable database when working with a multi-tenant set-up. The CDB name must be given via database.dbname in this case.","group":"Oracle","width":"MEDIUM","display_name":"PDB name","dependents":[],"order":8},"value":{"name":"database.pdb.name","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"database.out.server.name","type":"STRING","required":false,"default_value":null,"importance":"HIGH","documentation":"Name of the XStream Out server to connect to.","group":"Oracle","width":"MEDIUM","display_name":"XStream out server name","dependents":[],"order":9},"value":{"name":"database.out.server.name","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"snapshot.mode","type":"STRING","required":false,"default_value":"initial","importance":"LOW","documentation":"The criteria for running a snapshot upon startup of the connector. Select one of the following snapshot options: 'always': The connector runs a snapshot every time that it starts. After the snapshot completes, the connector begins to stream changes from the redo logs.; 'initial' (default): If the connector does not detect any offsets for the logical server name, it runs a snapshot that captures the current full state of the configured tables. After the snapshot completes, the connector begins to stream changes from the redo logs. 'initial_only': The connector performs a snapshot as it does for the 'initial' option, but after the connector completes the snapshot, it stops, and does not stream changes from the redo logs.; 'schema_only': If the connector does not detect any offsets for the logical server name, it runs a snapshot that captures only the schema (table structures), but not any table data. After the snapshot completes, the connector begins to stream changes from the redo logs.; 'schema_only_recovery': The connector performs a snapshot that captures only the database schema history. The connector then transitions to streaming from the redo logs. Use this setting to restore a corrupted or lost database schema history topic. Do not use if the database schema was modified after the connector stopped.","group":"Oracle","width":"SHORT","display_name":"Snapshot mode","dependents":[],"order":10},"value":{"name":"snapshot.mode","value":null,"recommended_values":["always","initial_only","configuration_based","when_needed","initial","custom","schema_only","no_data","recovery","schema_only_recovery"],"errors":[],"visible":true}},{"definition":{"name":"database.connection.adapter","type":"STRING","required":false,"default_value":"LogMiner","importance":"HIGH","documentation":"The adapter to use when capturing changes from the database. Options include: 'logminer': (the default) to capture changes using native Oracle LogMiner; 'xstream' to capture changes using Oracle XStreams","group":"Oracle","width":"MEDIUM","display_name":"Connector adapter","dependents":[],"order":11},"value":{"name":"database.connection.adapter","value":null,"recommended_values":["olr","xstream","logminer"],"errors":[],"visible":true}},{"definition":{"name":"log.mining.strategy","type":"STRING","required":false,"default_value":"redo_log_catalog","importance":"HIGH","documentation":"There are strategies: Online catalog with faster mining but no captured DDL. Another - with data dictionary loaded into REDO LOG files","group":"Oracle","width":"MEDIUM","display_name":"Log Mining Strategy","dependents":[],"order":12},"value":{"name":"log.mining.strategy","value":null,"recommended_values":["redo_log_catalog","hybrid","online_catalog"],"errors":[],"visible":true}},{"definition":{"name":"database.url","type":"STRING","required":false,"default_value":null,"importance":"HIGH","documentation":"Complete JDBC URL as an alternative to specifying hostname, port and database provided as a way to support alternative connection scenarios.","group":"Oracle","width":"LONG","display_name":"Complete JDBC URL","dependents":[],"order":13},"value":{"name":"database.url","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"event.processing.failure.handling.mode","type":"STRING","required":false,"default_value":"fail","importance":"MEDIUM","documentation":"Specify how failures during processing of events (i.e. when encountering a corrupted event) should be handled, including: 'fail' (the default) an exception indicating the problematic event and its position is raised, causing the connector to be stopped; 'warn' the problematic event and its position will be logged and the event will be skipped; 'ignore' the problematic event will be skipped.","group":"Connector","width":"SHORT","display_name":"Event deserialization failure handling","dependents":[],"order":1},"value":{"name":"event.processing.failure.handling.mode","value":null,"recommended_values":["warn","fail","ignore","skip"],"errors":[],"visible":true}},{"definition":{"name":"max.batch.size","type":"INT","required":false,"default_value":"2048","importance":"MEDIUM","documentation":"Maximum size of each batch of source records. Defaults to 2048.","group":"Connector","width":"SHORT","display_name":"Change event batch size","dependents":[],"order":2},"value":{"name":"max.batch.size","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"max.queue.size","type":"INT","required":false,"default_value":"8192","importance":"MEDIUM","documentation":"Maximum size of the queue for change events read from the database log but not yet recorded or forwarded. Defaults to 8192, and should always be larger than the maximum batch size.","group":"Connector","width":"SHORT","display_name":"Change event buffer size","dependents":[],"order":3},"value":{"name":"max.queue.size","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"poll.interval.ms","type":"LONG","required":false,"default_value":"500","importance":"MEDIUM","documentation":"Time to wait for new change events to appear after receiving no events, given in milliseconds. Defaults to 500 ms.","group":"Connector","width":"SHORT","display_name":"Poll interval (ms)","dependents":[],"order":4},"value":{"name":"poll.interval.ms","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"max.queue.size.in.bytes","type":"LONG","required":false,"default_value":"0","importance":"MEDIUM","documentation":"Maximum size of the queue in bytes for change events read from the database log but not yet recorded or forwarded. Defaults to 0. Mean the feature is not enabled","group":"Connector","width":"LONG","display_name":"Change event buffer size in bytes","dependents":[],"order":5},"value":{"name":"max.queue.size.in.bytes","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"provide.transaction.metadata","type":"BOOLEAN","required":false,"default_value":"false","importance":"LOW","documentation":"Enables transaction metadata extraction together with event counting","group":"Connector","width":"SHORT","display_name":"Store transaction metadata information in a dedicated topic.","dependents":[],"order":6},"value":{"name":"provide.transaction.metadata","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"skipped.operations","type":"LIST","required":false,"default_value":"t","importance":"LOW","documentation":"The comma-separated list of operations to skip during streaming, defined as: 'c' for inserts/create; 'u' for updates; 'd' for deletes, 't' for truncates, and 'none' to indicate nothing skipped. By default, only truncate operations will be skipped.","group":"Connector","width":"SHORT","display_name":"Skipped Operations","dependents":[],"order":7},"value":{"name":"skipped.operations","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"snapshot.delay.ms","type":"LONG","required":false,"default_value":"0","importance":"LOW","documentation":"A delay period before a snapshot will begin, given in milliseconds. Defaults to 0 ms.","group":"Connector","width":"MEDIUM","display_name":"Snapshot Delay (milliseconds)","dependents":[],"order":8},"value":{"name":"snapshot.delay.ms","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"streaming.delay.ms","type":"LONG","required":false,"default_value":"0","importance":"LOW","documentation":"A delay period after the snapshot is completed and the streaming begins, given in milliseconds. Defaults to 0 ms.","group":"Connector","width":"MEDIUM","display_name":"Streaming Delay (milliseconds)","dependents":[],"order":9},"value":{"name":"streaming.delay.ms","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"snapshot.include.collection.list","type":"LIST","required":false,"default_value":null,"importance":"MEDIUM","documentation":"This setting must be set to specify a list of tables/collections whose snapshot must be taken on creating or restarting the connector.","group":"Connector","width":"LONG","display_name":"Snapshot mode include data collection","dependents":[],"order":10},"value":{"name":"snapshot.include.collection.list","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"snapshot.fetch.size","type":"INT","required":false,"default_value":null,"importance":"MEDIUM","documentation":"The maximum number of records that should be loaded into memory while performing a snapshot.","group":"Connector","width":"MEDIUM","display_name":"Snapshot fetch size","dependents":[],"order":11},"value":{"name":"snapshot.fetch.size","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"snapshot.max.threads","type":"INT","required":false,"default_value":"1","importance":"MEDIUM","documentation":"The maximum number of threads used to perform the snapshot. Defaults to 1.","group":"Connector","width":"SHORT","display_name":"Snapshot maximum threads","dependents":[],"order":12},"value":{"name":"snapshot.max.threads","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"snapshot.mode.custom.name","type":"STRING","required":false,"default_value":null,"importance":"MEDIUM","documentation":"When 'snapshot.mode' is set as custom, this setting must be set to specify a the name of the custom implementation provided in the 'name()' method. The implementations must implement the 'Snapshotter' interface and is called on each app boot to determine whether to do a snapshot.","group":"Connector","width":"MEDIUM","display_name":"Snapshot Mode Custom Name","dependents":[],"order":13},"value":{"name":"snapshot.mode.custom.name","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"snapshot.mode.configuration.based.snapshot.data","type":"BOOLEAN","required":false,"default_value":"false","importance":"MEDIUM","documentation":"When 'snapshot.mode' is set as configuration_based, this setting permits to specify whenever the data should be snapshotted or not.","group":"Connector","width":"MEDIUM","display_name":"Snapshot mode property based snapshot data","dependents":[],"order":14},"value":{"name":"snapshot.mode.configuration.based.snapshot.data","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"snapshot.mode.configuration.based.snapshot.schema","type":"BOOLEAN","required":false,"default_value":"false","importance":"MEDIUM","documentation":"When 'snapshot.mode' is set as configuration_based, this setting permits to specify whenever the schema should be snapshotted or not.","group":"Connector","width":"MEDIUM","display_name":"Snapshot mode property based snapshot schema","dependents":[],"order":15},"value":{"name":"snapshot.mode.configuration.based.snapshot.schema","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"snapshot.mode.configuration.based.start.stream","type":"BOOLEAN","required":false,"default_value":"false","importance":"MEDIUM","documentation":"When 'snapshot.mode' is set as configuration_based, this setting permits to specify whenever the stream should start or not after snapshot.","group":"Connector","width":"MEDIUM","display_name":"Snapshot mode property based start stream","dependents":[],"order":16},"value":{"name":"snapshot.mode.configuration.based.start.stream","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"snapshot.mode.configuration.based.snapshot.on.schema.error","type":"BOOLEAN","required":false,"default_value":"false","importance":"MEDIUM","documentation":"When 'snapshot.mode' is set as configuration_based, this setting permits to specify whenever the schema should be snapshotted or not in case of error.","group":"Connector","width":"MEDIUM","display_name":"Snapshot mode property based snapshot on schema error","dependents":[],"order":17},"value":{"name":"snapshot.mode.configuration.based.snapshot.on.schema.error","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"snapshot.mode.configuration.based.snapshot.on.data.error","type":"BOOLEAN","required":false,"default_value":"false","importance":"MEDIUM","documentation":"When 'snapshot.mode' is set as configuration_based, this setting permits to specify whenever the data should be snapshotted or not in case of error.","group":"Connector","width":"MEDIUM","display_name":"Snapshot mode property based snapshot on data error","dependents":[],"order":18},"value":{"name":"snapshot.mode.configuration.based.snapshot.on.data.error","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"retriable.restart.connector.wait.ms","type":"LONG","required":false,"default_value":"10000","importance":"LOW","documentation":"Time to wait before restarting connector after retriable exception occurs. Defaults to 10000ms.","group":"Connector","width":"MEDIUM","display_name":"Retriable restart wait (ms)","dependents":[],"order":19},"value":{"name":"retriable.restart.connector.wait.ms","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"errors.max.retries","type":"INT","required":false,"default_value":"-1","importance":"LOW","documentation":"The maximum number of retries on connection errors before failing (-1 = no limit, 0 = disabled, > 0 = num of retries).","group":"Connector","width":"MEDIUM","display_name":"The maximum number of retries","dependents":[],"order":20},"value":{"name":"errors.max.retries","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"incremental.snapshot.watermarking.strategy","type":"STRING","required":false,"default_value":"INSERT_INSERT","importance":"LOW","documentation":"Specify the strategy used for watermarking during an incremental snapshot: 'insert_insert' both open and close signal is written into signal data collection (default); 'insert_delete' only open signal is written on signal data collection, the close will delete the relative open signal;","group":"Connector","width":"MEDIUM","display_name":"Incremental snapshot watermarking strategy","dependents":[],"order":21},"value":{"name":"incremental.snapshot.watermarking.strategy","value":null,"recommended_values":["insert_delete","insert_insert"],"errors":[],"visible":true}},{"definition":{"name":"internal.log.position.check.enable","type":"BOOLEAN","required":false,"default_value":"true","importance":"MEDIUM","documentation":"When enabled the connector checks if the position stored in the offset is still available in the log","group":"Connector","width":"MEDIUM","display_name":"Enable/Disable log position check","dependents":[],"order":22},"value":{"name":"internal.log.position.check.enable","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"decimal.handling.mode","type":"STRING","required":false,"default_value":"precise","importance":"MEDIUM","documentation":"Specify how DECIMAL and NUMERIC columns should be represented in change events, including: 'precise' (the default) uses java.math.BigDecimal to represent values, which are encoded in the change events using a binary representation and Kafka Connect's 'org.apache.kafka.connect.data.Decimal' type; 'string' uses string to represent values; 'double' represents values using Java's 'double', which may not offer the precision but will be far easier to use in consumers.","group":"Connector","width":"SHORT","display_name":"Decimal Handling","dependents":[],"order":23},"value":{"name":"decimal.handling.mode","value":null,"recommended_values":["string","double","precise"],"errors":[],"visible":true}},{"definition":{"name":"time.precision.mode","type":"STRING","required":false,"default_value":"adaptive","importance":"MEDIUM","documentation":"Time, date, and timestamps can be represented with different kinds of precisions, including: 'adaptive' (the default) bases the precision of time, date, and timestamp values on the database column's precision; 'adaptive_time_microseconds' like 'adaptive' mode, but TIME fields always use microseconds precision; 'connect' always represents time, date, and timestamp values using Kafka Connect's built-in representations for Time, Date, and Timestamp, which uses millisecond precision regardless of the database columns' precision.","group":"Connector","width":"SHORT","display_name":"Time Precision","dependents":[],"order":24},"value":{"name":"time.precision.mode","value":null,"recommended_values":["adaptive","adaptive_time_microseconds","connect"],"errors":[],"visible":true}},{"definition":{"name":"snapshot.lock.timeout.ms","type":"LONG","required":false,"default_value":"10000","importance":"MEDIUM","documentation":"The maximum number of millis to wait for table locks at the beginning of a snapshot. If locks cannot be acquired in this time frame, the snapshot will be aborted. Defaults to 10 seconds","group":"Connector","width":"LONG","display_name":"Snapshot lock timeout (ms)","dependents":[],"order":25},"value":{"name":"snapshot.lock.timeout.ms","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"query.fetch.size","type":"INT","required":false,"default_value":"10000","importance":"MEDIUM","documentation":"The maximum number of records that should be loaded into memory while streaming. A value of '0' uses the default JDBC fetch size, defaults to '2000'.","group":"Connector","width":"MEDIUM","display_name":"Query fetch size","dependents":[],"order":26},"value":{"name":"query.fetch.size","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"internal.snapshot.enhance.predicate.scn","type":"STRING","required":false,"default_value":null,"importance":"HIGH","documentation":"A token to replace on snapshot predicate template","group":"Connector","width":"MEDIUM","display_name":"A string to replace on snapshot predicate enhancement","dependents":[],"order":27},"value":{"name":"internal.snapshot.enhance.predicate.scn","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"snapshot.locking.mode","type":"STRING","required":false,"default_value":"shared","importance":"LOW","documentation":"Controls how the connector holds locks on tables while performing the schema snapshot. The default is 'shared', which means the connector will hold a table lock that prevents exclusive table access for just the initial portion of the snapshot while the database schemas and other metadata are being read. The remaining work in a snapshot involves selecting all rows from each table, and this is done using a flashback query that requires no locks. However, in some cases it may be desirable to avoid locks entirely which can be done by specifying 'none'. This mode is only safe to use if no schema changes are happening while the snapshot is taken.","group":"Connector","width":"SHORT","display_name":"Snapshot locking mode","dependents":[],"order":28},"value":{"name":"snapshot.locking.mode","value":null,"recommended_values":["shared","custom","none"],"errors":[],"visible":true}},{"definition":{"name":"rac.nodes","type":"STRING","required":false,"default_value":null,"importance":"HIGH","documentation":"A comma-separated list of RAC node hostnames or ip addresses","group":"Connector","width":"SHORT","display_name":"Oracle RAC nodes","dependents":[],"order":29},"value":{"name":"rac.nodes","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"interval.handling.mode","type":"STRING","required":false,"default_value":"numeric","importance":"LOW","documentation":"Specify how INTERVAL columns should be represented in change events, including: 'string' represents values as an exact ISO formatted string; 'numeric' (default) represents values using the inexact conversion into microseconds","group":"Connector","width":"MEDIUM","display_name":"Interval Handling","dependents":[],"order":30},"value":{"name":"interval.handling.mode","value":null,"recommended_values":["string","numeric"],"errors":[],"visible":true}},{"definition":{"name":"log.mining.archive.log.hours","type":"LONG","required":false,"default_value":"0","importance":"LOW","documentation":"The number of hours in the past from SYSDATE to mine archive logs. Using 0 mines all available archive logs","group":"Connector","width":"SHORT","display_name":"Log Mining Archive Log Hours","dependents":[],"order":31},"value":{"name":"log.mining.archive.log.hours","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"archive.log.hours","type":"LONG","required":false,"default_value":"0","importance":"LOW","documentation":"The number of hours in the past from SYSDATE to mine archive logs. Using 0 mines all available archive logs","group":"Connector","width":"SHORT","display_name":"Archive Log Hours","dependents":[],"order":32},"value":{"name":"archive.log.hours","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"log.mining.batch.size.default","type":"LONG","required":false,"default_value":"20000","importance":"LOW","documentation":"The starting SCN interval size that the connector will use for reading data from redo/archive logs.","group":"Connector","width":"SHORT","display_name":"Default batch size for reading redo/archive logs.","dependents":[],"order":33},"value":{"name":"log.mining.batch.size.default","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"log.mining.batch.size.min","type":"LONG","required":false,"default_value":"1000","importance":"LOW","documentation":"The minimum SCN interval size that this connector will try to read from redo/archive logs. Active batch size will be also increased/decreased by this amount for tuning connector throughput when needed.","group":"Connector","width":"SHORT","display_name":"Minimum batch size for reading redo/archive logs.","dependents":[],"order":34},"value":{"name":"log.mining.batch.size.min","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"log.mining.batch.size.max","type":"LONG","required":false,"default_value":"100000","importance":"LOW","documentation":"The maximum SCN interval size that this connector will use when reading from redo/archive logs.","group":"Connector","width":"SHORT","display_name":"Maximum batch size for reading redo/archive logs.","dependents":[],"order":35},"value":{"name":"log.mining.batch.size.max","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"log.mining.sleep.time.default.ms","type":"LONG","required":false,"default_value":"1000","importance":"LOW","documentation":"The amount of time that the connector will sleep after reading data from redo/archive logs and before starting reading data again. Value is in milliseconds.","group":"Connector","width":"SHORT","display_name":"Default sleep time in milliseconds when reading redo/archive logs.","dependents":[],"order":36},"value":{"name":"log.mining.sleep.time.default.ms","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"log.mining.sleep.time.min.ms","type":"LONG","required":false,"default_value":"0","importance":"LOW","documentation":"The minimum amount of time that the connector will sleep after reading data from redo/archive logs and before starting reading data again. Value is in milliseconds.","group":"Connector","width":"SHORT","display_name":"Minimum sleep time in milliseconds when reading redo/archive logs.","dependents":[],"order":37},"value":{"name":"log.mining.sleep.time.min.ms","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"log.mining.sleep.time.max.ms","type":"LONG","required":false,"default_value":"3000","importance":"LOW","documentation":"The maximum amount of time that the connector will sleep after reading data from redo/archive logs and before starting reading data again. Value is in milliseconds.","group":"Connector","width":"SHORT","display_name":"Maximum sleep time in milliseconds when reading redo/archive logs.","dependents":[],"order":38},"value":{"name":"log.mining.sleep.time.max.ms","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"log.mining.sleep.time.increment.ms","type":"LONG","required":false,"default_value":"200","importance":"LOW","documentation":"The maximum amount of time that the connector will use to tune the optimal sleep time when reading data from LogMiner. Value is in milliseconds.","group":"Connector","width":"SHORT","display_name":"The increment in sleep time in milliseconds used to tune auto-sleep behavior.","dependents":[],"order":39},"value":{"name":"log.mining.sleep.time.increment.ms","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"log.mining.transaction.retention.hours","type":"LONG","required":false,"default_value":"0","importance":"MEDIUM","documentation":"Hours to keep long running transactions in transaction buffer between log mining sessions. By default, all transactions are retained.","group":"Connector","width":"SHORT","display_name":"Log Mining long running transaction retention","dependents":[],"order":40},"value":{"name":"log.mining.transaction.retention.hours","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"log.mining.transaction.retention.ms","type":"LONG","required":false,"default_value":"0","importance":"MEDIUM","documentation":"Duration in milliseconds to keep long running transactions in transaction buffer between log mining sessions. By default, all transactions are retained.","group":"Connector","width":"SHORT","display_name":"Log Mining long running transaction retention","dependents":[],"order":41},"value":{"name":"log.mining.transaction.retention.ms","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"log.mining.archive.log.only.mode","type":"BOOLEAN","required":false,"default_value":"false","importance":"LOW","documentation":"When set to 'false', the default, the connector will mine both archive log and redo logs to emit change events. When set to 'true', the connector will only mine archive logs. There are circumstances where its advantageous to only mine archive logs and accept latency in event emission due to frequent revolving redo logs.","group":"Connector","width":"SHORT","display_name":"Specifies whether log mining should only target archive logs or both archive and redo logs","dependents":[],"order":42},"value":{"name":"log.mining.archive.log.only.mode","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"lob.enabled","type":"BOOLEAN","required":false,"default_value":"false","importance":"LOW","documentation":"When set to 'false', the default, LOB fields will not be captured nor emitted. When set to 'true', the connector will capture LOB fields and emit changes for those fields like any other column type.","group":"Connector","width":"SHORT","display_name":"Specifies whether the connector supports mining LOB fields and operations","dependents":[],"order":43},"value":{"name":"lob.enabled","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"log.mining.username.include.list","type":"STRING","required":false,"default_value":null,"importance":"LOW","documentation":"Comma separated list of usernames to include from LogMiner query.","group":"Connector","width":"SHORT","display_name":"List of users to include from LogMiner query","dependents":[],"order":44},"value":{"name":"log.mining.username.include.list","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"log.mining.username.exclude.list","type":"STRING","required":false,"default_value":null,"importance":"LOW","documentation":"Comma separated list of usernames to exclude from LogMiner query.","group":"Connector","width":"SHORT","display_name":"List of users to exclude from LogMiner query","dependents":[],"order":45},"value":{"name":"log.mining.username.exclude.list","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"log.mining.archive.destination.name","type":"STRING","required":false,"default_value":null,"importance":"LOW","documentation":"Sets the specific archive log destination as the source for reading archive logs.When not set, the connector will automatically select the first LOCAL and VALID destination.","group":"Connector","width":"MEDIUM","display_name":"Name of the archive log destination to be used for reading archive logs","dependents":[],"order":46},"value":{"name":"log.mining.archive.destination.name","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"archive.destination.name","type":"STRING","required":false,"default_value":null,"importance":"LOW","documentation":"Sets the specific archive log destination as the source for reading archive logs.When not set, the connector will automatically select the first LOCAL and VALID destination.","group":"Connector","width":"MEDIUM","display_name":"Name of the archive log destination to be used for reading archive logs","dependents":[],"order":47},"value":{"name":"archive.destination.name","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"log.mining.buffer.type","type":"STRING","required":false,"default_value":"memory","importance":"LOW","documentation":"The buffer type controls how the connector manages buffering transaction data.\n\nmemory - Uses the JVM process' heap to buffer all transaction data.\n\ninfinispan_embedded - This option uses an embedded Infinispan cache to buffer transaction data and persist it to disk.\n\ninfinispan_remote - This option uses a remote Infinispan cluster to buffer transaction data and persist it to disk.","group":"Connector","width":"NONE","display_name":"Controls which buffer type implementation to be used","dependents":[],"order":48},"value":{"name":"log.mining.buffer.type","value":null,"recommended_values":["memory","infinispan_embedded","infinispan_remote"],"errors":[],"visible":true}},{"definition":{"name":"log.mining.buffer.drop.on.stop","type":"BOOLEAN","required":false,"default_value":"false","importance":"LOW","documentation":"When set to true the underlying buffer cache is not retained when the connector is stopped. When set to false (the default), the buffer cache is retained across restarts.","group":"Connector","width":"SHORT","display_name":"Controls whether the buffer cache is dropped when connector is stopped","dependents":[],"order":49},"value":{"name":"log.mining.buffer.drop.on.stop","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"log.mining.buffer.infinispan.cache.global","type":"STRING","required":false,"default_value":null,"importance":"LOW","documentation":"Specifies the XML configuration for the Infinispan 'global' configuration","group":"Connector","width":"LONG","display_name":"Infinispan 'global' cache configuration","dependents":[],"order":50},"value":{"name":"log.mining.buffer.infinispan.cache.global","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"log.mining.buffer.infinispan.cache.transactions","type":"STRING","required":false,"default_value":null,"importance":"LOW","documentation":"Specifies the XML configuration for the Infinispan 'transactions' cache","group":"Connector","width":"LONG","display_name":"Infinispan 'transactions' cache configuration","dependents":[],"order":51},"value":{"name":"log.mining.buffer.infinispan.cache.transactions","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"log.mining.buffer.infinispan.cache.events","type":"STRING","required":false,"default_value":null,"importance":"LOW","documentation":"Specifies the XML configuration for the Infinispan 'events' cache","group":"Connector","width":"LONG","display_name":"Infinispan 'events' cache configurations","dependents":[],"order":52},"value":{"name":"log.mining.buffer.infinispan.cache.events","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"log.mining.buffer.infinispan.cache.processed_transactions","type":"STRING","required":false,"default_value":null,"importance":"LOW","documentation":"Specifies the XML configuration for the Infinispan 'processed-transactions' cache","group":"Connector","width":"LONG","display_name":"Infinispan 'processed-transactions' cache configuration","dependents":[],"order":53},"value":{"name":"log.mining.buffer.infinispan.cache.processed_transactions","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"log.mining.buffer.infinispan.cache.schema_changes","type":"STRING","required":false,"default_value":null,"importance":"LOW","documentation":"Specifies the XML configuration for the Infinispan 'schema-changes' cache","group":"Connector","width":"LONG","display_name":"Infinispan 'schema-changes' cache configuration","dependents":[],"order":54},"value":{"name":"log.mining.buffer.infinispan.cache.schema_changes","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"log.mining.buffer.transaction.events.threshold","type":"LONG","required":false,"default_value":"0","importance":"LOW","documentation":"The number of events a transaction can include before the transaction is discarded. This is useful for managing buffer memory and/or space when dealing with very large transactions. Defaults to 0, meaning that no threshold is applied and transactions can have unlimited events.","group":"Connector","width":"SHORT","display_name":"The maximum number of events a transaction can have before being discarded.","dependents":[],"order":55},"value":{"name":"log.mining.buffer.transaction.events.threshold","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"log.mining.archive.log.only.scn.poll.interval.ms","type":"LONG","required":false,"default_value":"10000","importance":"LOW","documentation":"The interval in milliseconds to wait between polls checking to see if the SCN is in the archive logs.","group":"Connector","width":"SHORT","display_name":"The interval in milliseconds to wait between polls when SCN is not yet in the archive logs","dependents":[],"order":56},"value":{"name":"log.mining.archive.log.only.scn.poll.interval.ms","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"log.mining.scn.gap.detection.gap.size.min","type":"LONG","required":false,"default_value":"1000000","importance":"LOW","documentation":"Used for SCN gap detection, if the difference between current SCN and previous end SCN is bigger than this value, and the time difference of current SCN and previous end SCN is smaller than log.mining.scn.gap.detection.time.interval.max.ms, consider it a SCN gap.","group":"Connector","width":"SHORT","display_name":"SCN gap size used to detect SCN gap","dependents":[],"order":57},"value":{"name":"log.mining.scn.gap.detection.gap.size.min","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"log.mining.scn.gap.detection.time.interval.max.ms","type":"LONG","required":false,"default_value":"20000","importance":"LOW","documentation":"Used for SCN gap detection, if the difference between current SCN and previous end SCN is bigger than log.mining.scn.gap.detection.gap.size.min, and the time difference of current SCN and previous end SCN is smaller than  this value, consider it a SCN gap.","group":"Connector","width":"SHORT","display_name":"Timer interval used to detect SCN gap","dependents":[],"order":58},"value":{"name":"log.mining.scn.gap.detection.time.interval.max.ms","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"unavailable.value.placeholder","type":"STRING","required":false,"default_value":"__debezium_unavailable_value","importance":"MEDIUM","documentation":"Specify the constant that will be provided by Debezium to indicate that the original value is unavailable and not provided by the database.","group":"Connector","width":"MEDIUM","display_name":"Unavailable value placeholder","dependents":[],"order":59},"value":{"name":"unavailable.value.placeholder","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"binary.handling.mode","type":"STRING","required":false,"default_value":"bytes","importance":"LOW","documentation":"Specify how binary (blob, binary, etc.) columns should be represented in change events, including: 'bytes' represents binary data as byte array (default); 'base64' represents binary data as base64-encoded string; 'base64-url-safe' represents binary data as base64-url-safe-encoded string; 'hex' represents binary data as hex-encoded (base16) string","group":"Connector","width":"MEDIUM","display_name":"Binary Handling","dependents":[],"order":60},"value":{"name":"binary.handling.mode","value":null,"recommended_values":["bytes","base64","hex","base64-url-safe"],"errors":[],"visible":true}},{"definition":{"name":"schema.name.adjustment.mode","type":"STRING","required":false,"default_value":"none","importance":"LOW","documentation":"Specify how schema names should be adjusted for compatibility with the message converter used by the connector, including: 'avro' replaces the characters that cannot be used in the Avro type name with underscore; 'avro_unicode' replaces the underscore or characters that cannot be used in the Avro type name with corresponding unicode like _uxxxx. Note: _ is an escape sequence like backslash in Java;'none' does not apply any adjustment (default)","group":"Connector","width":"MEDIUM","display_name":"Schema Name Adjustment","dependents":[],"order":61},"value":{"name":"schema.name.adjustment.mode","value":null,"recommended_values":["none","avro_unicode","avro"],"errors":[],"visible":true}},{"definition":{"name":"internal.log.mining.log.query.max.retries","type":"INT","required":false,"default_value":"5","importance":"LOW","documentation":"The maximum number of log query retries before throwing an exception that logs cannot be found.","group":"Connector","width":"SHORT","display_name":"Maximum number of retries before failing to locate redo logs","dependents":[],"order":62},"value":{"name":"internal.log.mining.log.query.max.retries","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"internal.log.mining.log.backoff.initial.delay.ms","type":"LONG","required":false,"default_value":"1000","importance":"LOW","documentation":"The initial delay when trying to query database redo logs, given in milliseconds. Defaults to 1 second (1,000 ms).","group":"Connector","width":"SHORT","display_name":"Initial delay when logs cannot yet be found (ms)","dependents":[],"order":63},"value":{"name":"internal.log.mining.log.backoff.initial.delay.ms","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"internal.log.mining.log.backoff.max.delay.ms","type":"LONG","required":false,"default_value":"60000","importance":"LOW","documentation":"The maximum delay when trying to query database redo logs, given in milliseconds. Defaults to 60 seconds (60,000 ms).","group":"Connector","width":"SHORT","display_name":"Maximum delay when logs cannot yet be found (ms)","dependents":[],"order":64},"value":{"name":"internal.log.mining.log.backoff.max.delay.ms","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"log.mining.session.max.ms","type":"LONG","required":false,"default_value":"0","importance":"LOW","documentation":"The maximum number of milliseconds that a LogMiner session lives for before being restarted. Defaults to 0 (indefinite until a log switch occurs)","group":"Connector","width":"SHORT","display_name":"Maximum number of milliseconds of a single LogMiner session","dependents":[],"order":65},"value":{"name":"log.mining.session.max.ms","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"internal.log.mining.transaction.snapshot.boundary.mode","type":"STRING","required":false,"default_value":"skip","importance":"LOW","documentation":"Specifies how in-progress transactions are to be handled when resolving the snapshot SCN. \nall - Captures in-progress transactions from both V$TRANSACTION and starting a LogMiner session near the snapshot SCN.\ntransaction_view_only - Captures in-progress transactions based on data in V$TRANSACTION only. Recently committed transactions near the flashback query SCN won't be included in the snapshot nor streaming.\nskip - Skips gathering any in-progress transactions.","group":"Connector","width":"SHORT","display_name":null,"dependents":[],"order":66},"value":{"name":"internal.log.mining.transaction.snapshot.boundary.mode","value":null,"recommended_values":["all","transaction_view_only","skip"],"errors":[],"visible":true}},{"definition":{"name":"internal.log.mining.read.only","type":"BOOLEAN","required":false,"default_value":"false","importance":"LOW","documentation":"When set to 'true', the connector will not attempt to flush the LGWR buffer to disk, allowing connecting to read-only databases.","group":"Connector","width":"SHORT","display_name":"Runs the connector in read-only mode","dependents":[],"order":67},"value":{"name":"internal.log.mining.read.only","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"log.mining.flush.table.name","type":"STRING","required":false,"default_value":"LOG_MINING_FLUSH","importance":"LOW","documentation":"The name of the flush table used by the connector, defaults to LOG_MINING_FLUSH.","group":"Connector","width":"MEDIUM","display_name":"Specifies the name of the flush table used by the connector","dependents":[],"order":68},"value":{"name":"log.mining.flush.table.name","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"log.mining.query.filter.mode","type":"STRING","required":false,"default_value":"none","importance":"MEDIUM","documentation":"Specifies how the filter configuration is applied to the LogMiner database query. \nnone - The query does not apply any schema or table filters, all filtering is at runtime by the connector.\nin - The query uses SQL in-clause expressions to specify the schema or table filters.\nregex - The query uses Oracle REGEXP_LIKE expressions to specify the schema or table filters.\n","group":"Connector","width":"SHORT","display_name":"Specifies how the filter configuration is applied to the LogMiner database query","dependents":[],"order":69},"value":{"name":"log.mining.query.filter.mode","value":null,"recommended_values":["regex","in","none"],"errors":[],"visible":true}},{"definition":{"name":"log.mining.restart.connection","type":"BOOLEAN","required":false,"default_value":"false","importance":"LOW","documentation":"Debezium opens a database connection and keeps that connection open throughout the entire streaming phase. In some situations, this can lead to excessive SGA memory usage. By setting this option to 'true' (the default is 'false'), the connector will close and re-open a database connection after every detected log switch or if the log.mining.session.max.ms has been reached.","group":"Connector","width":"SHORT","display_name":"Restarts Oracle database connection when reaching maximum session time or database log switch","dependents":[],"order":70},"value":{"name":"log.mining.restart.connection","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"internal.log.mining.max.scn.deviation.ms","type":"LONG","required":false,"default_value":"0","importance":"LOW","documentation":"By default, LogMiner will apply no deviation, meaning that the connector can mine up to the CURRENT_SCN. There are situations where this could be problematic if perhaps when asynchronous IO operations are at play. By applying a time-based deviation, for example 3000, the connector will only mine up the SCN that is a result of the formula of TIMESTAMP_TO_SCN(SCN_TO_TIMESTAMP(CURRENT_SCN)-(3000/86400000)). If this SCN is not available, the connector will log a warning and proceed to use the CURRENT_SCN or previously calculated upper SCN regardless. NOTE: This option is internal and should not be used for general use. Using this option will create a net latency on change events increased by the deviation value specified.","group":"Connector","width":"MEDIUM","display_name":"Allows applying a time-based deviation to the max mining scn","dependents":[],"order":71},"value":{"name":"internal.log.mining.max.scn.deviation.ms","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"internal.log.mining.schema_changes.username.exclude.list","type":"STRING","required":false,"default_value":"SYS,SYSTEM","importance":"LOW","documentation":"A comma-separated list of usernames that schema changes will be skipped for. Defaults to 'SYS,SYSTEM'.","group":"Connector","width":"MEDIUM","display_name":"Username exclusion list for schema changes","dependents":[],"order":72},"value":{"name":"internal.log.mining.schema_changes.username.exclude.list","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"log.mining.include.redo.sql","type":"BOOLEAN","required":false,"default_value":"false","importance":"LOW","documentation":"When enabled, the transaction log REDO SQL will be included in the source information block.","group":"Connector","width":"SHORT","display_name":"Include the transaction log SQL","dependents":[],"order":73},"value":{"name":"log.mining.include.redo.sql","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"openlogreplicator.source","type":"STRING","required":false,"default_value":null,"importance":"LOW","documentation":"The configured logical source name in the OpenLogReplicator configuration that is to stream changes","group":"Connector","width":"SHORT","display_name":"The logical source to stream changes from","dependents":[],"order":74},"value":{"name":"openlogreplicator.source","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"openlogreplicator.host","type":"STRING","required":false,"default_value":null,"importance":"LOW","documentation":"The hostname of the OpenLogReplicator network service","group":"Connector","width":"MEDIUM","display_name":"The hostname of the OpenLogReplicator network service","dependents":[],"order":75},"value":{"name":"openlogreplicator.host","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"openlogreplicator.port","type":"INT","required":false,"default_value":null,"importance":"LOW","documentation":"The port of the OpenLogReplicator network service","group":"Connector","width":"MEDIUM","display_name":"The port of the OpenLogReplicator network service","dependents":[],"order":76},"value":{"name":"openlogreplicator.port","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"snapshot.database.errors.max.retries","type":"INT","required":false,"default_value":"0","importance":"LOW","documentation":"The number of attempts to retry database errors during snapshots before failing.","group":"Connector","width":"SHORT","display_name":"The maximum number of retries before snapshot database errors are not retried","dependents":[],"order":77},"value":{"name":"snapshot.database.errors.max.retries","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"log.mining.continuous.mine","type":"BOOLEAN","required":false,"default_value":"false","importance":"LOW","documentation":"(Deprecated) if true, CONTINUOUS_MINE option will be added to the log mining session. This will manage log files switches seamlessly.","group":"Connector","width":"SHORT","display_name":"Should log mining session configured with CONTINUOUS_MINE setting?","dependents":[],"order":78},"value":{"name":"log.mining.continuous.mine","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"internal.object.id.cache.size","type":"INT","required":false,"default_value":"256","importance":"LOW","documentation":"The connector maintains a least-recently used cache of database table object ID to name mappings. This controls the maximum capacity of this cache.","group":"Connector","width":"SHORT","display_name":"Controls the maximum size of the object ID cache","dependents":[],"order":79},"value":{"name":"internal.object.id.cache.size","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"schema.history.internal","type":"CLASS","required":false,"default_value":"io.debezium.storage.kafka.history.KafkaSchemaHistory","importance":"LOW","documentation":"The name of the SchemaHistory class that should be used to store and recover database schema changes. The configuration properties for the history are prefixed with the 'schema.history.internal.' string.","group":"History Storage","width":"LONG","display_name":"Database schema history class","dependents":[],"order":1},"value":{"name":"schema.history.internal","value":null,"recommended_values":[],"errors":[],"visible":false}},{"definition":{"name":"schema.history.internal.skip.unparseable.ddl","type":"BOOLEAN","required":false,"default_value":"false","importance":"LOW","documentation":"Controls the action Debezium will take when it meets a DDL statement in binlog, that it cannot parse.By default the connector will stop operating but by changing the setting it can ignore the statements which it cannot parse. If skipping is enabled then Debezium can miss metadata changes.","group":"History Storage","width":"SHORT","display_name":"Skip DDL statements that cannot be parsed","dependents":[],"order":2},"value":{"name":"schema.history.internal.skip.unparseable.ddl","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"schema.history.internal.store.only.captured.tables.ddl","type":"BOOLEAN","required":false,"default_value":"false","importance":"LOW","documentation":"Controls what DDL will Debezium store in database schema history. By default (false) Debezium will store all incoming DDL statements. If set to true, then only DDL that manipulates a captured table will be stored.","group":"History Storage","width":"SHORT","display_name":"Store only DDL that modifies tables that are captured based on include/exclude lists","dependents":[],"order":3},"value":{"name":"schema.history.internal.store.only.captured.tables.ddl","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"schema.history.internal.store.only.captured.databases.ddl","type":"BOOLEAN","required":false,"default_value":"false","importance":"LOW","documentation":"Controls what DDL will Debezium store in database schema history. By default (true) only DDL that manipulates a table from captured schema/database will be stored. If set to false, then Debezium will store all incoming DDL statements.","group":"History Storage","width":"SHORT","display_name":"Store only DDL that modifies tables of databases that are captured based on include/exclude lists","dependents":[],"order":4},"value":{"name":"schema.history.internal.store.only.captured.databases.ddl","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"converters","type":"STRING","required":false,"default_value":null,"importance":"LOW","documentation":"Optional list of custom converters that would be used instead of default ones. The converters are defined using '<converter.prefix>.type' config option and configured using options '<converter.prefix>.<option>'","group":"Events","width":"MEDIUM","display_name":"List of prefixes defining custom values converters.","dependents":[],"order":1},"value":{"name":"converters","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"post.processors","type":"STRING","required":false,"default_value":null,"importance":"LOW","documentation":"Optional list of post processors. The processors are defined using '<post.processor.prefix>.type' config option and configured using options '<post.processor.prefix.<option>'","group":"Events","width":"MEDIUM","display_name":"List of change event post processors.","dependents":[],"order":2},"value":{"name":"post.processors","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"tombstones.on.delete","type":"BOOLEAN","required":false,"default_value":"true","importance":"MEDIUM","documentation":"Whether delete operations should be represented by a delete event and a subsequent tombstone event (true) or only by a delete event (false). Emitting the tombstone event (the default behavior) allows Kafka to completely delete all events pertaining to the given key once the source record got deleted.","group":"Events","width":"SHORT","display_name":"Change the behaviour of Debezium with regards to delete operations","dependents":[],"order":3},"value":{"name":"tombstones.on.delete","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"heartbeat.interval.ms","type":"INT","required":false,"default_value":"0","importance":"MEDIUM","documentation":"Length of an interval in milli-seconds in in which the connector periodically sends heartbeat messages to a heartbeat topic. Use 0 to disable heartbeat messages. Disabled by default.","group":"Events","width":"MEDIUM","display_name":"Connector heartbeat interval (milli-seconds)","dependents":[],"order":4},"value":{"name":"heartbeat.interval.ms","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"heartbeat.topics.prefix","type":"STRING","required":false,"default_value":"__debezium-heartbeat","importance":"LOW","documentation":"The prefix that is used to name heartbeat topics.Defaults to __debezium-heartbeat.","group":"Events","width":"MEDIUM","display_name":"A prefix used for naming of heartbeat topics","dependents":[],"order":5},"value":{"name":"heartbeat.topics.prefix","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"signal.data.collection","type":"STRING","required":false,"default_value":null,"importance":"MEDIUM","documentation":"The name of the data collection that is used to send signals/commands to Debezium. Signaling is disabled when not set.","group":"Events","width":"MEDIUM","display_name":"Signaling data collection","dependents":[],"order":6},"value":{"name":"signal.data.collection","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"signal.poll.interval.ms","type":"LONG","required":false,"default_value":"5000","importance":"MEDIUM","documentation":"Interval for looking for new signals in registered channels, given in milliseconds. Defaults to 5 seconds.","group":"Events","width":"SHORT","display_name":"Signal processor poll interval","dependents":[],"order":7},"value":{"name":"signal.poll.interval.ms","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"signal.enabled.channels","type":"LIST","required":false,"default_value":"source","importance":"MEDIUM","documentation":"List of channels names that are enabled. Source channel is enabled by default","group":"Events","width":"LONG","display_name":"Enabled channels names","dependents":[],"order":8},"value":{"name":"signal.enabled.channels","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"topic.naming.strategy","type":"CLASS","required":false,"default_value":"io.debezium.schema.SchemaTopicNamingStrategy","importance":"MEDIUM","documentation":"The name of the TopicNamingStrategy class that should be used to determine the topic name for data change, schema change, transaction, heartbeat event etc.","group":"Events","width":"MEDIUM","display_name":"Topic naming strategy class","dependents":[],"order":9},"value":{"name":"topic.naming.strategy","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"notification.enabled.channels","type":"LIST","required":false,"default_value":null,"importance":"MEDIUM","documentation":"List of notification channels names that are enabled.","group":"Events","width":"LONG","display_name":"Enabled notification channels names","dependents":[],"order":10},"value":{"name":"notification.enabled.channels","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"notification.sink.topic.name","type":"STRING","required":false,"default_value":null,"importance":"HIGH","documentation":"The name of the topic for the notifications. This is required in case 'sink' is in the list of enabled channels","group":"Events","width":"LONG","display_name":"Notification topic name","dependents":[],"order":11},"value":{"name":"notification.sink.topic.name","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"transaction.metadata.factory","type":"CLASS","required":false,"default_value":"io.debezium.pipeline.txmetadata.DefaultTransactionMetadataFactory","importance":"LOW","documentation":"Class to make transaction context & transaction struct/schemas","group":"Events","width":"MEDIUM","display_name":"Factory class to create transaction context & transaction struct maker classes","dependents":[],"order":12},"value":{"name":"transaction.metadata.factory","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"custom.metric.tags","type":"LIST","required":false,"default_value":null,"importance":"LOW","documentation":"The custom metric tags will accept key-value pairs to customize the MBean object name which should be appended the end of regular name, each key would represent a tag for the MBean object name, and the corresponding value would be the value of that tag the key is. For example: k1=v1,k2=v2","group":"Events","width":"MEDIUM","display_name":"Customize metric tags","dependents":[],"order":13},"value":{"name":"custom.metric.tags","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"column.include.list","type":"LIST","required":false,"default_value":null,"importance":"MEDIUM","documentation":"Regular expressions matching columns to include in change events","group":"Events","width":"LONG","display_name":"Include Columns","dependents":[],"order":14},"value":{"name":"column.include.list","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"column.exclude.list","type":"LIST","required":false,"default_value":null,"importance":"MEDIUM","documentation":"Regular expressions matching columns to exclude from change events","group":"Events","width":"LONG","display_name":"Exclude Columns","dependents":[],"order":15},"value":{"name":"column.exclude.list","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"table.include.list","type":"LIST","required":false,"default_value":null,"importance":"HIGH","documentation":"The tables for which changes are to be captured","group":"Events","width":"LONG","display_name":"Include Tables","dependents":[],"order":16},"value":{"name":"table.include.list","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"table.exclude.list","type":"LIST","required":false,"default_value":null,"importance":"MEDIUM","documentation":"A comma-separated list of regular expressions that match the fully-qualified names of tables to be excluded from monitoring","group":"Events","width":"LONG","display_name":"Exclude Tables","dependents":[],"order":17},"value":{"name":"table.exclude.list","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"message.key.columns","type":"STRING","required":false,"default_value":null,"importance":"MEDIUM","documentation":"A semicolon-separated list of expressions that match fully-qualified tables and column(s) to be used as message key. Each expression must match the pattern '<fully-qualified table name>:<key columns>', where the table names could be defined as (DB_NAME.TABLE_NAME) or (SCHEMA_NAME.TABLE_NAME), depending on the specific connector, and the key columns are a comma-separated list of columns representing the custom key. For any table without an explicit key configuration the table's primary key column(s) will be used as message key. Example: dbserver1.inventory.orderlines:orderId,orderLineId;dbserver1.inventory.orders:id","group":"Events","width":"LONG","display_name":"Columns PK mapping","dependents":[],"order":18},"value":{"name":"message.key.columns","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"snapshot.select.statement.overrides","type":"STRING","required":false,"default_value":null,"importance":"MEDIUM","documentation":" This property contains a comma-separated list of fully-qualified tables (DB_NAME.TABLE_NAME) or (SCHEMA_NAME.TABLE_NAME), depending on the specific connectors. Select statements for the individual tables are specified in further configuration properties, one for each table, identified by the id 'snapshot.select.statement.overrides.[DB_NAME].[TABLE_NAME]' or 'snapshot.select.statement.overrides.[SCHEMA_NAME].[TABLE_NAME]', respectively. The value of those properties is the select statement to use when retrieving data from the specific table during snapshotting. A possible use case for large append-only tables is setting a specific point where to start (resume) snapshotting, in case a previous snapshotting was interrupted.","group":"Events","width":"LONG","display_name":"List of tables where the default select statement used during snapshotting should be overridden.","dependents":[],"order":19},"value":{"name":"snapshot.select.statement.overrides","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"column.mask.hash.([^.]+).with.salt.(.+)","type":"STRING","required":false,"default_value":null,"importance":"MEDIUM","documentation":"A comma-separated list of regular expressions matching fully-qualified names of columns that should be masked by hashing the input. Using the specified hash algorithms and salt.","group":"Events","width":"LONG","display_name":"Mask Columns Using Hash and Salt","dependents":[],"order":20},"value":{"name":"column.mask.hash.([^.]+).with.salt.(.+)","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"column.mask.with.(d+).chars","type":"STRING","required":false,"default_value":null,"importance":"MEDIUM","documentation":"A comma-separated list of regular expressions matching fully-qualified names of columns that should be masked with configured amount of asterisk ('*') characters.","group":"Events","width":"NONE","display_name":"Mask Columns With n Asterisks","dependents":[],"order":21},"value":{"name":"column.mask.with.(d+).chars","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"column.truncate.to.(d+).chars","type":"INT","required":false,"default_value":null,"importance":"MEDIUM","documentation":"A comma-separated list of regular expressions matching fully-qualified names of columns that should be truncated to the configured amount of characters.","group":"Events","width":"NONE","display_name":"Truncate Columns To n Characters","dependents":[],"order":22},"value":{"name":"column.truncate.to.(d+).chars","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"include.schema.changes","type":"BOOLEAN","required":false,"default_value":"true","importance":"MEDIUM","documentation":"Whether the connector should publish changes in the database schema to a Kafka topic with the same name as the database server ID. Each schema change will be recorded using a key that contains the database name and whose value include logical description of the new schema and optionally the DDL statement(s). The default is 'true'. This is independent of how the connector internally records database schema history.","group":"Events","width":"SHORT","display_name":"Include database schema changes","dependents":[],"order":23},"value":{"name":"include.schema.changes","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"include.schema.comments","type":"BOOLEAN","required":false,"default_value":"false","importance":"MEDIUM","documentation":"Whether the connector parse table and column's comment to metadata object. Note: Enable this option will bring the implications on memory usage. The number and size of ColumnImpl objects is what largely impacts how much memory is consumed by the Debezium connectors, and adding a String to each of them can potentially be quite heavy. The default is 'false'.","group":"Events","width":"SHORT","display_name":"Include Table and Column Comments","dependents":[],"order":24},"value":{"name":"include.schema.comments","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"column.propagate.source.type","type":"LIST","required":false,"default_value":null,"importance":"MEDIUM","documentation":"A comma-separated list of regular expressions matching fully-qualified names of columns that adds the columnâ€™s original type and original length as parameters to the corresponding field schemas in the emitted change records.","group":"Events","width":"NONE","display_name":"Propagate Source Types by Columns","dependents":[],"order":25},"value":{"name":"column.propagate.source.type","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"datatype.propagate.source.type","type":"LIST","required":false,"default_value":null,"importance":"MEDIUM","documentation":"A comma-separated list of regular expressions matching the database-specific data type names that adds the data type's original type and original length as parameters to the corresponding field schemas in the emitted change records.","group":"Events","width":"NONE","display_name":"Propagate Source Types by Data Type","dependents":[],"order":26},"value":{"name":"datatype.propagate.source.type","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"internal.snapshot.scan.all.columns.force","type":"BOOLEAN","required":false,"default_value":"false","importance":"LOW","documentation":"Restore pre 1.5 behaviour and scan all tables to discover columns. If you are excluding one table then turning this on may improve performance. If you are excluding a lot of tables the default behavior should work well.","group":"Events","width":"SHORT","display_name":"Snapshot force scan all columns of all tables","dependents":[],"order":27},"value":{"name":"internal.snapshot.scan.all.columns.force","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"snapshot.tables.order.by.row.count","type":"STRING","required":false,"default_value":"disabled","importance":"MEDIUM","documentation":"Controls the order in which tables are processed in the initial snapshot. A `descending` value will order the tables by row count descending. A `ascending` value will order the tables by row count ascending. A value of `disabled` (the default) will disable ordering by row count.","group":"Events","width":"NONE","display_name":"Initial snapshot tables order by row count","dependents":[],"order":28},"value":{"name":"snapshot.tables.order.by.row.count","value":null,"recommended_values":["disabled","ascending","descending"],"errors":[],"visible":true}},{"definition":{"name":"heartbeat.action.query","type":"STRING","required":false,"default_value":null,"importance":"LOW","documentation":"The query executed with every heartbeat.","group":"Events","width":"MEDIUM","display_name":"An optional query to execute with every heartbeat","dependents":[],"order":29},"value":{"name":"heartbeat.action.query","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"sourceinfo.struct.maker","type":"CLASS","required":false,"default_value":"io.debezium.connector.oracle.OracleSourceInfoStructMaker","importance":"LOW","documentation":"The name of the SourceInfoStructMaker class that returns SourceInfo schema and struct.","group":"Events","width":"MEDIUM","display_name":"Source info struct maker class","dependents":[],"order":30},"value":{"name":"sourceinfo.struct.maker","value":null,"recommended_values":[],"errors":[],"visible":true}}]}