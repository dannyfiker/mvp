name: data-lakehouse-mvp

services:
  # ---------------- OZONE 2.0 (SCM + OM + DN + S3G) ----------------
  ozone-scm:
    image: apache/ozone:2.0.0
    container_name: mvp-ozone-scm
    command: ["ozone","scm"]
    environment:
      - OZONE_METADATA_DIRS=/data/ozone-meta/scm
      - OZONE_REPLICATION_FACTOR=1
    ports: ["9876:9876"]
    volumes:
      - ozone_data:/data
      - ../data-lakehouse/ozone/ozone-site.scm.xml:/opt/ozone/etc/hadoop/ozone-site.xml:ro
    networks: [dlx]

  ozone-om:
    image: apache/ozone:2.0.0
    container_name: mvp-ozone-om
    depends_on: [ozone-scm]
    command: ["ozone","om"]
    environment:
      - OZONE_METADATA_DIRS=/data/ozone-meta/om
      - OZONE_REPLICATION_FACTOR=1
    ports: ["9862:9862"]
    volumes:
      - ozone_data:/data
      - ../data-lakehouse/ozone/ozone-site.om.xml:/opt/ozone/etc/hadoop/ozone-site.xml:ro
    networks: [dlx]

  ozone-dn:
    image: apache/ozone:2.0.0
    container_name: mvp-ozone-dn
    depends_on: [ozone-om]
    command: ["ozone","datanode"]
    environment:
      - OZONE_METADATA_DIRS=/data/ozone-meta/dn
      - OZONE_REPLICATION_FACTOR=1
    volumes:
      - ozone_data:/data
      - ../data-lakehouse/ozone/ozone-site.dn.xml:/opt/ozone/etc/hadoop/ozone-site.xml:ro
    networks: [dlx]

  ozone-s3g:
    image: apache/ozone:2.0.0
    container_name: mvp-ozone-s3g
    depends_on: [ozone-om, ozone-dn]
    command: ["ozone","s3g"]
    environment:
      - OZONE_METADATA_DIRS=/data/ozone-meta/s3g
      - OZONE_REPLICATION_FACTOR=1
      - OZONE_S3G_HTTP_ADDRESS=0.0.0.0:9878
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=admin123
    ports: ["9878:9878"]
    volumes:
      - ozone_data:/data
      - ../data-lakehouse/ozone/ozone-site.s3g.xml:/opt/ozone/etc/hadoop/ozone-site.xml:ro
    networks: [dlx]

  ozone-exit-safemode:
    image: apache/ozone:2.0.0
    container_name: mvp-ozone-exit-safemode
    depends_on:
      - ozone-scm
      - ozone-om
      - ozone-dn
    entrypoint: ["/bin/bash","-lc"]
    command:
      - |
        set -e
        echo "checking SCM safemode..." >&2
        for i in $(seq 1 60); do
          if ozone admin safemode status 2>/dev/null | grep -q "out of safe mode"; then
            echo "SCM is out of safe mode." >&2
            exit 0
          fi
          ozone admin safemode exit >/dev/null 2>&1 || true
          sleep 2
        done
        echo "timed out waiting for SCM to exit safe mode; continuing" >&2
        ozone admin safemode status || true
        exit 0
    volumes:
      - ../data-lakehouse/ozone/ozone-site.scm.xml:/opt/ozone/etc/hadoop/ozone-site.xml:ro
    networks: [dlx]
    restart: "no"

  init-bucket:
    image: amazon/aws-cli:2.17.54
    container_name: mvp-init-bucket
    depends_on:
      ozone-exit-safemode:
        condition: service_completed_successfully
      ozone-s3g:
        condition: service_started
    environment:
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=admin123
      - AWS_DEFAULT_REGION=us-east-1
    entrypoint: ["/bin/sh","-lc"]
    command:
      - |
        set -e
        until aws --endpoint-url http://ozone-s3g:9878 s3 ls >/dev/null 2>&1; do
          echo "waiting for ozone-s3g..." >&2
          sleep 2
        done
        aws --endpoint-url http://ozone-s3g:9878 s3 mb s3://warehouse || true
        aws --endpoint-url http://ozone-s3g:9878 s3 ls
    networks: [dlx]
    restart: "no"

  # ---------------- APACHE KAFKA (KRaft, official image) ------------
  kafka:
    image: apache/kafka:4.1.0
    container_name: mvp-kafka
    ports:
      # Host-facing listener (so host consumers don't need to resolve 'mvp-kafka')
      - "19092:19092"
    environment:
       # --- KRaft single-node config ---
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@mvp-kafka:9093

      # --- Listeners ---
      # internal + external listeners
      KAFKA_LISTENERS: INTERNAL://:9092,EXTERNAL://:19092,CONTROLLER://:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT
      # what other containers/host use to reach broker
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://mvp-kafka:9092,EXTERNAL://localhost:19092
      # which listener is used for controller and inter-broker
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL

      # --- Log directory (where KRaft metadata + logs live) ---
      KAFKA_LOG_DIRS: /var/lib/kafka/data

      # Optional but handy in a dev MVP
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"

      # Single-broker settings for internal topics
      KAFKA_DEFAULT_REPLICATION_FACTOR: "1"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: "1"
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: "1"
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: "1"


    #  KAFKA_CFG_ADVERTISED_LISTENERS: PLAINTEXT://mvp-kafka:9092
    
    # Attach to dlx network and give it an alias "kafka"
    networks:
      dlx:
        aliases:
          - kafka
    volumes:
      # Persist Kafka data (the image uses /var/lib/kafka/data)
      - kafka_data:/var/lib/kafka/data
    healthcheck:
      test: ["CMD-SHELL", "/opt/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list >/dev/null 2>&1"]
      interval: 10s
      timeout: 5s
      retries: 30

  # ---------------- Kafbat UI (dynamic config) ----------------------
  kafbat-ui:
    image: kafbat/kafka-ui:main
    container_name: mvp-kafbat-ui
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      - DYNAMIC_CONFIG_ENABLED=true
    volumes:
      - ../data-lakehouse/kafbat/config.yaml:/etc/kafkaui/dynamic_config.yaml:ro
    ports: ["8080:8080"]
    networks: [dlx]

  # ---------------- Kafka Connect (Debezium; JSON) ------------------
  connect:
    build:
      context: ./connect
    image: mvp-connect:2.7.3.Final-iceberg
    container_name: mvp-connect
    user: "0:0"
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      - BOOTSTRAP_SERVERS=kafka:9092
      - GROUP_ID=1
      - CONFIG_STORAGE_TOPIC=connect_configs
      - OFFSET_STORAGE_TOPIC=connect_offsets
      - STATUS_STORAGE_TOPIC=connect_status
      - KAFKA_HEAP_OPTS=-Xms512m -Xmx1024m
      # Iceberg S3FileIO uses AWS SDK v2 which requires an explicit region.
      - AWS_REGION=us-east-1
      - AWS_DEFAULT_REGION=us-east-1
      # Make Apicurio converter/resolver JARs available to connector classloaders.
      # Debezium's entrypoint will symlink these libs into each plugin directory.
      - ENABLE_APICURIO_CONVERTERS=true
      # Default converters for the Connect worker. Individual connectors can override.
      # For this MVP we use value-only Avro: keys are schemaless JSON, values are Avro.
      - KEY_CONVERTER=org.apache.kafka.connect.json.JsonConverter
      - VALUE_CONVERTER=io.apicurio.registry.utils.converter.AvroConverter
      # Debezium image maps CONNECT_* vars into connect-distributed.properties.
      - CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE=false
      - CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE=false
      - CONNECT_VALUE_CONVERTER_APICURIO_REGISTRY_URL=http://apicurio:8080/apis/registry/v2
      - CONNECT_VALUE_CONVERTER_APICURIO_REGISTRY_AUTO_REGISTER=true
      - CONNECT_VALUE_CONVERTER_APICURIO_REGISTRY_FIND_LATEST=true
      # Make Avro payloads Confluent-compatible (magic byte + 4-byte schema id)
      # so Kafka UI can render Avro values as JSON.
      - CONNECT_VALUE_CONVERTER_APICURIO_REGISTRY_AS_CONFLUENT=true
      # Match Debezium + Streams SerDe default (see SerdeFactory): contentId + Legacy4ByteIdHandler.
      - CONNECT_VALUE_CONVERTER_APICURIO_REGISTRY_USE_ID=contentId
      - CONNECT_VALUE_CONVERTER_APICURIO_REGISTRY_ID_HANDLER=io.apicurio.registry.serde.Legacy4ByteIdHandler
    ports: ["8083:8083"]
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks: [dlx]

  # ---------------- Hive Metastore (Apache; Derby) ------------------
  hive-metastore:
    image: apache/hive:standalone-metastore-4.1.0
    container_name: mvp-hive-metastore
    environment:
      - SERVICE_NAME=metastore
    ports: ["9083:9083"]
    networks: [dlx]

  # ---------------- Iceberg REST Catalog ----------------------------
  iceberg-rest:
    image: tabulario/iceberg-rest:1.6.0
    container_name: mvp-iceberg-rest
    depends_on:
      - hive-metastore
      - ozone-s3g
    environment:
      - CATALOG_WAREHOUSE=s3://warehouse/
      - CATALOG_IO__IMPL=org.apache.iceberg.aws.s3.S3FileIO
      - AWS_REGION=us-east-1
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=admin123

      # Iceberg catalog/FileIO properties for S3-compatible stores (Ozone S3G).
      # This image maps env vars starting with CATALOG_ into catalog properties using:
      # - '_' => '.'
      # - '__' => '-'
      - CATALOG_S3_ENDPOINT=http://ozone-s3g:9878
      - CATALOG_S3_PATH__STYLE__ACCESS=true
      - CATALOG_S3_REGION=us-east-1
      - CATALOG_S3_ACCESS__KEY__ID=admin
      - CATALOG_S3_SECRET__ACCESS__KEY=admin123

      # Retain older variables for compatibility (harmless if ignored)
      - S3_ENDPOINT=http://ozone-s3g:9878
      - S3_PATH_STYLE_ACCESS=true
    ports:
      - "8181:8181"
    networks:
      dlx:
        aliases:
          - rest

  # ---------------- Trino (latest OSS) ------------------------------
  trino:
    image: trinodb/trino:477
    container_name: mvp-trino
    depends_on:
      - iceberg-rest
    volumes:
      - ../data-lakehouse/trino/etc:/etc/trino:ro
      - trino_data:/data/trino
    ports: ["8085:8080"]
    networks: [dlx]

  # ---------------- Spark + Iceberg (OSS quickstart) ----------------
  spark:
    image: tabulario/spark-iceberg:latest
    container_name: mvp-spark
    depends_on:
      - iceberg-rest
    environment:
      - SPARK_MODE=client
    volumes:
      - ../data-lakehouse/ozone/ozone-site.xml:/opt/spark/conf/ozone-site.xml:ro
      - ../data-lakehouse/ozone/core-site.xml:/opt/spark/conf/core-site.xml:ro
      - ../data-lakehouse/scripts/jobs:/opt/jobs
    networks: [dlx]

  # ---------------- Apicurio Schema Registry (kafkasql) -------------
  apicurio:
    image: quay.io/apicurio/apicurio-registry-kafkasql:2.5.10.Final
    container_name: mvp-apicurio
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      QUARKUS_PROFILE: prod
      REGISTRY_KAFKASQL_BOOTSTRAP_SERVERS: mvp-kafka:9092
      KAFKA_BOOTSTRAP_SERVERS: mvp-kafka:9092
      QUARKUS_KAFKA_BOOTSTRAP_SERVERS: mvp-kafka:9092
      QUARKUS_KAFKA_STREAMS_BOOTSTRAP_SERVERS: mvp-kafka:9092
      MP_MESSAGING_CONNECTOR_SMALLRYE_KAFKA_BOOTSTRAP_SERVERS: mvp-kafka:9092
      REGISTRY_STORAGE: kafkasql
      KAFKA_SECURITY_PROTOCOL: PLAINTEXT
    ports:
      - "8081:8080"
    healthcheck:
      test: ["CMD-SHELL","curl -fsS http://localhost:8080/health/ready >/dev/null || exit 1"]
      interval: 10s
      timeout: 3s
      retries: 15
    restart: unless-stopped
    networks: [dlx]

  # ---------------- Superset ----------------------------------------
  superset-db:
    image: postgres:16.4-alpine
    container_name: mvp-superset-db
    environment:
      POSTGRES_DB: superset
      POSTGRES_USER: superset
      POSTGRES_PASSWORD: superset
    volumes:
      - superset_db:/var/lib/postgresql/data
    restart: unless-stopped
    networks: [dlx]

  superset:
    # build:
    #   context: ../data-lakehouse/superset        # uses superset/Dockerfile
    image: mvp-superset:4.1.1-extended
    container_name: mvp-superset
    depends_on:
      - superset-db
    environment:
      SUPERSET_SECRET_KEY: "change-me-please-very-long"
      SUPERSET_DATABASE_URI: postgresql+psycopg2://superset:superset@superset-db:5432/superset
    volumes:
      - ./superset/superset_home:/app/superset_home
      - ./superset/config/superset_config.py:/app/pythonpath/superset_config.py
    ports:
      - "8088:8088"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    command:
      [
        "/bin/bash","-lc",
        "superset db upgrade && \
         superset fab create-admin --username admin --firstname Admin --lastname User --email admin@example.com --password admin || true && \
         superset init && \
         gunicorn --workers 2 --timeout 120 -b 0.0.0.0:8088 'superset.app:create_app()'"
      ]
    restart: unless-stopped
    networks: [dlx]

  # ---------------- Gov Aggregator -----------------------
  gov-aggregator:
    image: ${GOV_AGGREGATOR_IMAGE:-mvp-gov-aggregator:local}
    container_name: mvp-gov-aggregator
    depends_on:
      - kafka
      - apicurio
    environment:
      BOOTSTRAP_SERVERS: kafka:9092
      APICURIO_URL: http://apicurio:8080/apis/registry/v2
      APICURIO_GROUP_ID: gov-aggregator
      TOPIC_MOR: mor.tax.payments
      TOPIC_ECC: ecc.trade.permits
      TOPIC_MOTRI: motri.transport.permits
      TOPIC_NBE: nbe.fx.rates
      TOPIC_MOE: moe.education.stats
      TOPIC_OUT: gov.aggregates.enriched
    restart: unless-stopped
    networks: [dlx]

  # ---------------- Bronze (Debezium) -> Silver (Avro) --------------
  debezium-to-silver:
    image: ${GOV_AGGREGATOR_IMAGE:-mvp-gov-aggregator:local}
    container_name: mvp-debezium-to-silver
    depends_on:
      - kafka
      - connect
      - apicurio
    environment:
      MAIN_CLASS: et.gov.lakehouse.govaggregator.core.DebeziumToSilverApp
      BOOTSTRAP_SERVERS: kafka:9092
      # Bump app id to reprocess and populate the canonical silver topics.
      APPLICATION_ID: debezium-to-silver-oracle-esw-v2

      # Apicurio Registry (Avro)
      APICURIO_URL: http://apicurio:8080/apis/registry/v2
      BRONZE_APICURIO_GROUP_ID: oracle-esw
      SILVER_APICURIO_GROUP_ID: oracle-esw-silver

      # Debezium topics routed by RegexRouter: oracle_esw.ESW.<TABLE> -> raw-<TABLE>
      BRONZE_TOPICS: raw-TB_CB_LPCO,raw-TB_CB_LPCO_AMDT_ATTCH_DOC,raw-TB_CB_LPCO_ATTCH_DOC,raw-TB_CB_LPCO_CMDT,raw-TB_CB_LPCO_CMNT,raw-TB_CB_LPCO_CNCL_ATTCH_DOC,raw-TB_CB_LPCO_CSTMS,raw-TB_CB_LPCO_MPNG

      # Output topic naming: silver.oracle_esw.<TABLE>
      SILVER_TOPIC_PREFIX: silver.oracle_esw.
      SILVER_NAME_STYLE: full
      SILVER_STRIP_PREFIX: raw-
      SILVER_RECORD_NAMESPACE: silver.oracle_esw

      # Iceberg dynamic routing field uses this namespace.
      ICEBERG_NAMESPACE: silver

      # Keep false until you explicitly approve the mapping in connectors/silver-oracle-esw.approval.md
      SILVER_APPROVED: "true"
    restart: unless-stopped
    networks: [dlx]

  # ---------------- Silver (Kafka) -> Iceberg (tables) --------------
  silver-to-iceberg:
    image: tabulario/spark-iceberg:latest
    container_name: mvp-silver-to-iceberg
    depends_on:
      - kafka
      - iceberg-rest
      - ozone-s3g
    environment:
      - SPARK_MODE=client
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      # Only consume the canonical silver topics (exclude legacy silver.nbe_mock.*)
      - TOPIC_PATTERN=silver\.(accounts|banks|branches|customers|transactions|oracle_esw\..*)
      - ICEBERG_REST_URI=http://iceberg-rest:8181
      - WAREHOUSE_SILVER_LOCATION=s3://warehouse/silver/
      # Change checkpoint path to force a one-time backfill from earliest offsets.
      - CHECKPOINT_LOCATION=/tmp/checkpoints/kafka_silver_to_iceberg_backfill_v1
      - STARTING_OFFSETS=earliest
      # Pre-create tables so they show up in Trino/Superset even if early micro-batches are empty.
      # NOTE: kafka_silver_to_iceberg.py flattens topic suffixes by replacing '.' with '_'.
      - EXPECTED_TABLES=accounts,banks,branches,customers,transactions,oracle_esw_tb_cb_lpco_cmnt,oracle_esw_tb_cb_lpco_cstms,oracle_esw_tb_cb_lpco_attch_doc,oracle_esw_tb_cb_lpco_amdt_attch_doc,oracle_esw_tb_cb_lpco_mpng,oracle_esw_tb_cb_lpco_cncl_attch_doc,oracle_esw_tb_cb_lpco
      - S3_ENDPOINT=http://ozone-s3g:9878
      - S3_PATH_STYLE_ACCESS=true
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=admin123
      - AWS_REGION=us-east-1
    volumes:
      - ../data-lakehouse/ozone/ozone-site.xml:/opt/spark/conf/ozone-site.xml:ro
      - ../data-lakehouse/ozone/core-site.xml:/opt/spark/conf/core-site.xml:ro
      - ../data-lakehouse/scripts/jobs:/opt/jobs
      # Provide Kafka connector deps without using Ivy/--packages
      - ../data-lakehouse/spark-jars:/opt/extra-jars:ro
    # NOTE: /opt/spark/entrypoint.sh only runs `eval "$1"` (first arg), so pass a single string.
    # The base image does not include the Structured Streaming Kafka connector.
    command:
      - "spark-submit --jars /opt/extra-jars/spark-sql-kafka-0-10_2.12-3.5.5.jar,/opt/extra-jars/spark-token-provider-kafka-0-10_2.12-3.5.5.jar,/opt/extra-jars/kafka-clients-3.4.1.jar,/opt/extra-jars/commons-pool2-2.11.1.jar /opt/jobs/kafka_silver_to_iceberg.py"
    restart: unless-stopped
    networks: [dlx]

 # ---------------- NGINX reverse proxy (Superset + Phi-3) ----------
  nginx:
    image: nginx:1.28
    container_name: mvp-nginx
    depends_on:
      - superset
    ports:
      - "80:80"
    volumes:
      # nginx config file (see below)
      - ./nginx/superset.conf:/etc/nginx/conf.d/default.conf:ro
    extra_hosts:
      # so nginx can reach uvicorn on the host at http://host.docker.internal:8001
      - "host.docker.internal:host-gateway"
    networks:
      - dlx
  
    # ---------------- Monitoring: Prometheus + Grafana + cAdvisor -----
  prometheus:
    image: prom/prometheus:latest
    container_name: mvp-prometheus
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=15d"
      - "--web.enable-lifecycle"
    ports:
      - "9090:9090"
    networks: [dlx]
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    container_name: mvp-grafana
    depends_on:
      - prometheus
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_SERVER_DOMAIN=localhost
      - GF_SERVER_ROOT_URL=%(protocol)s://%(domain)s/
    volumes:
      - grafana_data:/var/lib/grafana
    ports:
      - "3000:3000"
    networks: [dlx]
    restart: unless-stopped

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    container_name: mvp-cadvisor
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
    ports:
      - "8089:8080"   # avoid conflict with other services
    networks: [dlx]
    restart: unless-stopped

  node-exporter:
    image: prom/node-exporter:latest
    container_name: mvp-node-exporter
    command:
      - '--path.rootfs=/host'
    volumes:
      - /:/host:ro,rslave
    ports:
      - "9100:9100"
    networks: [dlx]
    restart: unless-stopped

networks:
  dlx: {}

volumes:
  kafka_data:
  ozone_data:
  trino_data:
  superset_db:
  prometheus_data:
  grafana_data:
