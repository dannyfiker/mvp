name: data-lakehouse-mvp

services:
  # ---------------- OZONE 2.0 (SCM + OM + DN + S3G) ----------------
  ozone-scm:
    image: apache/ozone:2.0.0
    container_name: mvp-ozone-scm
    command: ["ozone","scm"]
    environment:
      - OZONE_METADATA_DIRS=/data/ozone-meta/scm
      - OZONE_REPLICATION_FACTOR=1
    ports: ["9876:9876"]
    volumes:
      - ozone_data:/data
      - ../data-lakehouse/ozone/ozone-site.scm.xml:/opt/ozone/etc/hadoop/ozone-site.xml:ro
    networks: [dlx]

  ozone-om:
    image: apache/ozone:2.0.0
    container_name: mvp-ozone-om
    depends_on: [ozone-scm]
    command: ["ozone","om"]
    environment:
      - OZONE_METADATA_DIRS=/data/ozone-meta/om
      - OZONE_REPLICATION_FACTOR=1
    ports: ["9862:9862"]
    volumes:
      - ozone_data:/data
      - ../data-lakehouse/ozone/ozone-site.om.xml:/opt/ozone/etc/hadoop/ozone-site.xml:ro
    networks: [dlx]

  ozone-dn:
    image: apache/ozone:2.0.0
    container_name: mvp-ozone-dn
    depends_on: [ozone-om]
    command: ["ozone","datanode"]
    environment:
      - OZONE_METADATA_DIRS=/data/ozone-meta/dn
      - OZONE_REPLICATION_FACTOR=1
    volumes:
      - ozone_data:/data
      - ../data-lakehouse/ozone/ozone-site.dn.xml:/opt/ozone/etc/hadoop/ozone-site.xml:ro
    networks: [dlx]

  ozone-s3g:
    image: apache/ozone:2.0.0
    container_name: mvp-ozone-s3g
    depends_on: [ozone-om, ozone-dn]
    command: ["ozone","s3g"]
    environment:
      - OZONE_METADATA_DIRS=/data/ozone-meta/s3g
      - OZONE_REPLICATION_FACTOR=1
      - OZONE_S3G_HTTP_ADDRESS=0.0.0.0:9878
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=admin123
    ports: ["9878:9878"]
    volumes:
      - ozone_data:/data
      - ../data-lakehouse/ozone/ozone-site.s3g.xml:/opt/ozone/etc/hadoop/ozone-site.xml:ro
    networks: [dlx]

  init-bucket:
    image: amazon/aws-cli:2.17.54
    container_name: mvp-init-bucket
    depends_on:
      - ozone-s3g
    environment:
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=admin123
      - AWS_DEFAULT_REGION=us-east-1
    entrypoint: ["/bin/sh","-lc"]
    command:
      - |
        set -e
        until aws --endpoint-url http://ozone-s3g:9878 s3 ls >/dev/null 2>&1; do
          echo "waiting for ozone-s3g..." >&2
          sleep 2
        done
        aws --endpoint-url http://ozone-s3g:9878 s3 mb s3://warehouse || true
        aws --endpoint-url http://ozone-s3g:9878 s3 ls
    networks: [dlx]
    restart: "no"

  # ---------------- APACHE KAFKA (KRaft, official image) ------------
  kafka:
    image: apache/kafka:4.1.0
    container_name: mvp-kafka
    ports:
      - "9092:9092"
    environment:
       # --- KRaft single-node config ---
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@mvp-kafka:9093

      # --- Listeners ---
      # internal listeners (inside container)
      KAFKA_LISTENERS: PLAINTEXT://:9092,CONTROLLER://:9093
      # what other containers/host use to reach broker
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://mvp-kafka:9092
      # which listener is used for controller and inter-broker
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT

      # --- Log directory (where KRaft metadata + logs live) ---
      KAFKA_LOG_DIRS: /var/lib/kafka/data

      # Optional but handy in a dev MVP
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"

      # Single-broker settings for internal topics
      KAFKA_DEFAULT_REPLICATION_FACTOR: "1"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: "1"
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: "1"
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: "1"


    #  KAFKA_CFG_ADVERTISED_LISTENERS: PLAINTEXT://mvp-kafka:9092
    
    # Attach to dlx network and give it an alias "kafka"
    networks:
      dlx:
        aliases:
          - kafka
    volumes:
      # Persist Kafka data (the image uses /var/lib/kafka/data)
      - kafka_data:/var/lib/kafka/data
    healthcheck:
      test: ["CMD-SHELL", "/opt/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list >/dev/null 2>&1"]
      interval: 10s
      timeout: 5s
      retries: 30

  # ---------------- Kafbat UI (dynamic config) ----------------------
  kafbat-ui:
    image: kafbat/kafka-ui:main
    container_name: mvp-kafbat-ui
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      - DYNAMIC_CONFIG_ENABLED=true
    volumes:
      - ../data-lakehouse/kafbat/config.yaml:/etc/kafkaui/dynamic_config.yaml:ro
    ports: ["8080:8080"]
    networks: [dlx]

  # ---------------- Kafka Connect (Debezium; JSON) ------------------
  connect:
    image: debezium/connect:2.7.3.Final
    container_name: mvp-connect
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      - BOOTSTRAP_SERVERS=kafka:9092
      - GROUP_ID=1
      - CONFIG_STORAGE_TOPIC=connect_configs
      - OFFSET_STORAGE_TOPIC=connect_offsets
      - STATUS_STORAGE_TOPIC=connect_status
      # Default converters for the Connect worker. Individual connectors can override.
      # For this MVP we use value-only Avro: keys are schemaless JSON, values are Avro.
      - KEY_CONVERTER=org.apache.kafka.connect.json.JsonConverter
      - KEY_CONVERTER_SCHEMAS_ENABLE=false
      - VALUE_CONVERTER=io.apicurio.registry.utils.converter.AvroConverter
      - VALUE_CONVERTER_APICURIO_REGISTRY_URL=http://apicurio:8080/apis/registry/v2
      - VALUE_CONVERTER_APICURIO_REGISTRY_AUTO_REGISTER=true
      - VALUE_CONVERTER_APICURIO_REGISTRY_FIND_LATEST=true
    ports: ["8083:8083"]
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      # Provide AvroConverter implementation on Connect plugin.path (/kafka/connect)
      - ../data-lakehouse/apps/gov-aggregator/target/gov-aggregator-0.1.0-SNAPSHOT.jar:/kafka/connect/apicurio-avro-converter/gov-aggregator-0.1.0-SNAPSHOT.jar:ro
    networks: [dlx]

  # ---------------- Hive Metastore (Apache; Derby) ------------------
  hive-metastore:
    image: apache/hive:standalone-metastore-4.1.0
    container_name: mvp-hive-metastore
    environment:
      - SERVICE_NAME=metastore
    ports: ["9083:9083"]
    networks: [dlx]

  # ---------------- Iceberg REST Catalog ----------------------------
  iceberg-rest:
    image: tabulario/iceberg-rest:1.6.0
    container_name: mvp-iceberg-rest
    depends_on:
      - hive-metastore
      - ozone-s3g
    environment:
      - CATALOG_WAREHOUSE=s3://warehouse/
      - CATALOG_IO__IMPL=org.apache.iceberg.aws.s3.S3FileIO
      - AWS_REGION=us-east-1
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=admin123

      # Iceberg catalog/FileIO properties for S3-compatible stores (Ozone S3G).
      # This image maps env vars starting with CATALOG_ into catalog properties using:
      # - '_' => '.'
      # - '__' => '-'
      - CATALOG_S3_ENDPOINT=http://ozone-s3g:9878
      - CATALOG_S3_PATH__STYLE__ACCESS=true
      - CATALOG_S3_REGION=us-east-1
      - CATALOG_S3_ACCESS__KEY__ID=admin
      - CATALOG_S3_SECRET__ACCESS__KEY=admin123

      # Retain older variables for compatibility (harmless if ignored)
      - S3_ENDPOINT=http://ozone-s3g:9878
      - S3_PATH_STYLE_ACCESS=true
    ports:
      - "8181:8181"
    networks:
      dlx:
        aliases:
          - rest

  # ---------------- Trino (latest OSS) ------------------------------
  trino:
    image: trinodb/trino:477
    container_name: mvp-trino
    depends_on:
      - iceberg-rest
    volumes:
      - ../data-lakehouse/trino/etc:/etc/trino:ro
      - trino_data:/data/trino
    ports: ["8085:8080"]
    networks: [dlx]

  # ---------------- Spark + Iceberg (OSS quickstart) ----------------
  spark:
    image: tabulario/spark-iceberg:latest
    container_name: mvp-spark
    depends_on:
      - iceberg-rest
    environment:
      - SPARK_MODE=client
    volumes:
      - ../data-lakehouse/ozone/ozone-site.xml:/opt/spark/conf/ozone-site.xml:ro
      - ../data-lakehouse/ozone/core-site.xml:/opt/spark/conf/core-site.xml:ro
      - ../data-lakehouse/scripts/jobs:/opt/jobs
    networks: [dlx]

  # ---------------- Apicurio Schema Registry (kafkasql) -------------
  apicurio:
    image: quay.io/apicurio/apicurio-registry-kafkasql:2.5.10.Final
    container_name: mvp-apicurio
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      QUARKUS_PROFILE: prod
      REGISTRY_KAFKASQL_BOOTSTRAP_SERVERS: mvp-kafka:9092
      KAFKA_BOOTSTRAP_SERVERS: mvp-kafka:9092
      QUARKUS_KAFKA_BOOTSTRAP_SERVERS: mvp-kafka:9092
      QUARKUS_KAFKA_STREAMS_BOOTSTRAP_SERVERS: mvp-kafka:9092
      MP_MESSAGING_CONNECTOR_SMALLRYE_KAFKA_BOOTSTRAP_SERVERS: mvp-kafka:9092
      REGISTRY_STORAGE: kafkasql
      KAFKA_SECURITY_PROTOCOL: PLAINTEXT
    ports:
      - "8081:8080"
    healthcheck:
      test: ["CMD-SHELL","wget -qO- http://localhost:8080/health/ready || exit 1"]
      interval: 10s
      timeout: 3s
      retries: 15
    restart: unless-stopped
    networks: [dlx]

  # ---------------- Superset ----------------------------------------
  superset-db:
    image: postgres:16.4-alpine
    container_name: mvp-superset-db
    environment:
      POSTGRES_DB: superset
      POSTGRES_USER: superset
      POSTGRES_PASSWORD: superset
    volumes:
      - superset_db:/var/lib/postgresql/data
    restart: unless-stopped
    networks: [dlx]

  superset:
    build:
      context: ../data-lakehouse/superset        # uses superset/Dockerfile
    image: mvp-superset:3.1.0-hive
    container_name: mvp-superset
    depends_on:
      - superset-db
    environment:
      SUPERSET_SECRET_KEY: "change-me-please-very-long"
      SUPERSET_DATABASE_URI: postgresql+psycopg2://superset:superset@superset-db:5432/superset
    volumes:
      - ./superset/superset_home:/app/superset_home
      - ./superset/config/superset_config.py:/app/pythonpath/superset_config.py
    ports:
      - "8088:8088"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    command:
      [
        "/bin/bash","-lc",
        "superset db upgrade && \
         superset fab create-admin --username admin --firstname Admin --lastname User --email admin@example.com --password admin || true && \
         superset init && \
         gunicorn --workers 2 --timeout 120 -b 0.0.0.0:8088 'superset.app:create_app()'"
      ]
    restart: unless-stopped
    networks: [dlx]

  # ---------------- Gov Aggregator (your app) -----------------------
  gov-aggregator:
    build:
      context: ../data-lakehouse/apps/gov-aggregator
      dockerfile: Dockerfile.runtime
    container_name: mvp-gov-aggregator
    depends_on:
      - kafka
      - apicurio
    environment:
      BOOTSTRAP_SERVERS: kafka:9092
      APICURIO_URL: http://apicurio:8080/apis/registry/v2
      APICURIO_GROUP_ID: gov-aggregator
      TOPIC_MOR: mor.tax.payments
      TOPIC_ECC: ecc.trade.permits
      TOPIC_MOTRI: motri.transport.permits
      TOPIC_NBE: nbe.fx.rates
      TOPIC_MOE: moe.education.stats
      TOPIC_OUT: gov.aggregates.enriched
    restart: unless-stopped
    networks: [dlx]

  # ---------------- Bronze (Debezium) -> Silver (JSON) --------------
  debezium-to-silver:
    build:
      context: ../data-lakehouse/apps/gov-aggregator
      dockerfile: Dockerfile.runtime
    container_name: mvp-debezium-to-silver
    depends_on:
      - kafka
      - connect
      - apicurio
    environment:
      MAIN_CLASS: et.gov.lakehouse.govaggregator.core.DebeziumToSilverApp
      BOOTSTRAP_SERVERS: kafka:9092
      # Bump app id to reprocess and populate the canonical silver.<table> topics.
      APPLICATION_ID: debezium-to-silver-v2

      # Apicurio Registry (Avro)
      APICURIO_URL: http://apicurio:8080/apis/registry/v2
      APICURIO_GROUP_ID: pg1

      # Debezium topics produced by your connector (topic.prefix=pg1)
      BRONZE_TOPICS: pg1.nbe_mock.accounts,pg1.nbe_mock.transactions,pg1.nbe_mock.customers,pg1.nbe_mock.branches,pg1.nbe_mock.banks

      # Output topic naming: silver.<input-topic> by default
      SILVER_TOPIC_PREFIX: silver.
      # Naming style:
      # - full (default): silver.<input-topic> (optionally with SILVER_STRIP_PREFIX)
      # - last-segment: silver.<table> (e.g. pg1.nbe_mock.accounts -> silver.accounts)
      SILVER_NAME_STYLE: last-segment
      SILVER_STRIP_PREFIX: ""
    restart: unless-stopped
    networks: [dlx]

  # ---------------- Silver (Kafka) -> Iceberg (tables) --------------
  silver-to-iceberg:
    image: tabulario/spark-iceberg:latest
    container_name: mvp-silver-to-iceberg
    depends_on:
      - kafka
      - iceberg-rest
      - ozone-s3g
    environment:
      - SPARK_MODE=client
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      # Only consume the canonical silver topics (exclude legacy silver.nbe_mock.*)
      - TOPIC_PATTERN=silver\.(accounts|banks|branches|customers|transactions)
      - ICEBERG_REST_URI=http://iceberg-rest:8181
      - WAREHOUSE_SILVER_LOCATION=s3://warehouse/silver/
      - CHECKPOINT_LOCATION=/tmp/checkpoints/kafka_silver_to_iceberg_v2
      - STARTING_OFFSETS=earliest
      - S3_ENDPOINT=http://ozone-s3g:9878
      - S3_PATH_STYLE_ACCESS=true
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=admin123
      - AWS_REGION=us-east-1
    volumes:
      - ../data-lakehouse/ozone/ozone-site.xml:/opt/spark/conf/ozone-site.xml:ro
      - ../data-lakehouse/ozone/core-site.xml:/opt/spark/conf/core-site.xml:ro
      - ../data-lakehouse/scripts/jobs:/opt/jobs
      # Provide Kafka connector deps without using Ivy/--packages
      - ../data-lakehouse/spark-jars:/opt/extra-jars:ro
    # NOTE: /opt/spark/entrypoint.sh only runs `eval "$1"` (first arg), so pass a single string.
    # The base image does not include the Structured Streaming Kafka connector.
    command:
      - "spark-submit --jars /opt/extra-jars/spark-sql-kafka-0-10_2.12-3.5.5.jar,/opt/extra-jars/spark-token-provider-kafka-0-10_2.12-3.5.5.jar,/opt/extra-jars/kafka-clients-3.4.1.jar,/opt/extra-jars/commons-pool2-2.11.1.jar /opt/jobs/kafka_silver_to_iceberg.py"
    restart: unless-stopped
    networks: [dlx]

 # ---------------- NGINX reverse proxy (Superset + Phi-3) ----------
  nginx:
    image: nginx:1.28
    container_name: mvp-nginx
    depends_on:
      - superset
    ports:
      - "80:80"
    volumes:
      # nginx config file (see below)
      - ./nginx/superset.conf:/etc/nginx/conf.d/default.conf:ro
    extra_hosts:
      # so nginx can reach uvicorn on the host at http://host.docker.internal:8001
      - "host.docker.internal:host-gateway"
    networks:
      - dlx
  
    # ---------------- Monitoring: Prometheus + Grafana + cAdvisor -----
  prometheus:
    image: prom/prometheus:latest
    container_name: mvp-prometheus
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=15d"
      - "--web.enable-lifecycle"
    ports:
      - "9090:9090"
    networks: [dlx]
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    container_name: mvp-grafana
    depends_on:
      - prometheus
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_SERVER_DOMAIN=localhost
      - GF_SERVER_ROOT_URL=%(protocol)s://%(domain)s/
    volumes:
      - grafana_data:/var/lib/grafana
    ports:
      - "3000:3000"
    networks: [dlx]
    restart: unless-stopped

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    container_name: mvp-cadvisor
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
    ports:
      - "8089:8080"   # avoid conflict with other services
    networks: [dlx]
    restart: unless-stopped

  node-exporter:
    image: prom/node-exporter:latest
    container_name: mvp-node-exporter
    command:
      - '--path.rootfs=/host'
    volumes:
      - /:/host:ro,rslave
    ports:
      - "9100:9100"
    networks: [dlx]
    restart: unless-stopped

networks:
  dlx: {}

volumes:
  kafka_data:
  ozone_data:
  trino_data:
  superset_db:
  prometheus_data:
  grafana_data:
